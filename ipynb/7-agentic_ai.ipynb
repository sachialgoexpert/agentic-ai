{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd6fc4f7",
   "metadata": {},
   "source": [
    "### Building chatbot with multiple tools using langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3978ca10",
   "metadata": {},
   "source": [
    "### Create a chatbot with tool capabilities with arxiv, wikipedia search and some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68d5ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph,START,END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23a8290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"]=os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "440f133f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Sachchida, nice to meet you. It's great to hear that you're an AI enthusiast. There have been so many advancements in the field of Artificial Intelligence recently, from natural language processing to computer vision. What specific areas of AI interest you the most? Are you exploring any AI-related projects or learning about AI through courses or books?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 46, 'total_tokens': 117, 'completion_time': 0.215613866, 'prompt_time': 0.002251057, 'queue_time': 0.050442942, 'total_time': 0.217864923}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_34d416ee39', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--ee4acaf2-02be-4bcc-b9e8-2cfad41d6242-0', usage_metadata={'input_tokens': 46, 'output_tokens': 71, 'total_tokens': 117})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm_groq=ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "llm_groq.invoke(\"hi I am sachchida. I am AI enthusiast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1dbd2cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "954ccedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv\n"
     ]
    }
   ],
   "source": [
    "api_wrapper_arxiv=ArxivAPIWrapper(top_k_results=2, doc_content_chars_max=500)\n",
    "arxiv=ArxivQueryRun(api_wrapper=api_wrapper_arxiv)\n",
    "print(arxiv.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "992f85fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Published: 2024-07-22\\nTitle: Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\\nAuthors: Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini\\nSummary: The inference demand for LLMs has skyrocketed in recent months, and serving\\nmodels with low latencies remains challenging due to the quadratic input length\\ncomplexity of the attention layers. In this work, we investigate the effect of\\ndropping MLP and attention layers at inference time o\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv.invoke(\"Attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b34e09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia\n"
     ]
    }
   ],
   "source": [
    "api_wrapper_wiki=WikipediaAPIWrapper(top_k_results=2, doc_content_chars_max=500)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper_wiki)\n",
    "print(wiki.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39d67ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: Barack Obama\\nSummary: Barack Hussein Obama II (born August 4, 1961) is an American politician who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, he was the first African American president. Obama previously served as a U.S. senator representing Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004.\\nBorn in Honolulu, Hawaii, Obama graduated from Columbia University in 1983 with a Bachelor of Arts degree in polit'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.invoke(\"barack obama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4796726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily=TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b94f70d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '1 USD to INR - US Dollars to Indian Rupees Exchange Rate - Xe',\n",
       "  'url': 'https://www.xe.com/en-us/currencyconverter/convert/?Amount=1&From=USD&To=INR',\n",
       "  'content': '# 1 USD to INR - Convert US Dollars to Indian Rupees\\n\\nXe Currency Converter\\n\\nusd\\ninr\\n\\n1.00 US Dollar =\\n\\n87.91523 Indian Rupee\\n\\n1 INR = 0.0113746 USD\\n\\n### Looking to make large transfers?\\n\\nWe can beat competitor rates\\n\\nWe use the mid-market rate for our Converter. This is for informational purposes only. You won’t receive this rate when sending money. Login to view send rates\\n\\n#### Convert US Dollar to Indian Rupee\\n\\nRate information of USD/INR currency pair [...] | inr Indian RupeeINR | usd US DollarUSD |\\n --- |\\n| 1 INR | 0.0113746 USD |\\n| 5 INR | 0.056873 USD |\\n| 10 INR | 0.113746 USD |\\n| 25 INR | 0.284365 USD |\\n| 50 INR | 0.56873 USD |\\n| 100 INR | 1.13746 USD |\\n| 500 INR | 5.6873 USD |\\n| 1,000 INR | 11.3746 USD |\\n| 5,000 INR | 56.873 USD |\\n| 10,000 INR | 113.746 USD |\\n\\ninr\\nusd\\n\\n## USD to INR chart\\n\\n## USD to INR Chart\\n\\n1 USD = 0 INR\\n\\n1 USD to INR exchange-rate stats: high, low, average, and volatility over the last 7, 30, and 90 days. [...] | usd US DollarUSD | inr Indian RupeeINR |\\n --- |\\n| 1 USD | 87.9152 INR |\\n| 5 USD | 439.576 INR |\\n| 10 USD | 879.152 INR |\\n| 25 USD | 2,197.88 INR |\\n| 50 USD | 4,395.76 INR |\\n| 100 USD | 8,791.52 INR |\\n| 500 USD | 43,957.6 INR |\\n| 1,000 USD | 87,915.2 INR |\\n| 5,000 USD | 439,576 INR |\\n| 10,000 USD | 879,152 INR |\\n\\nusd\\ninr\\n\\n#### Convert Indian Rupee to US Dollar\\n\\nRate information of INR/USD currency pair',\n",
       "  'score': 0.86441374},\n",
       " {'title': 'Indian Rupee - Quote - Chart - Historical Data - News',\n",
       "  'url': 'https://tradingeconomics.com/india/currency',\n",
       "  'content': \"## The USD/INR exchange rate rose to 87.8870 on October 16, 2025, up 0.08% from the previous session. Over the past month, the Indian Rupee has strengthened 0.03%, but it's down by 4.57% over the last 12 months. Historically, the USDINR reached an all time high of 88.97 in September of 2025. Indian Rupee - data, forecasts, historical chart - was last updated on October 16 of 2025. [...] ### The USD/INR exchange rate rose to 87.8870 on October 16, 2025, up 0.08% from the previous session. Over the past month, the Indian Rupee has strengthened 0.03%, but it's down by 4.57% over the last 12 months. The Indian Rupee is expected to trade at 88.48 by the end of this quarter, according to Trading Economics global macro models and analysts expectations. Looking forward, we estimate it to trade at 87.52 in 12 months time. [...] ## The Indian rupee traded around 87.8 per USD, remaining near a one-month high after the Reserve Bank of India took aggressive steps to counter heavy selling pressure. After holding the currency above its record low of 88.8 for the past two weeks, the central bank took a more forceful stance to curb depreciation, conducting heavy dollar sales through state-run banks. The intervention helped ease the strains that had weighed on the rupee in recent weeks, including steep US tariffs, tighter\",\n",
       "  'score': 0.8539252},\n",
       " {'title': 'USD to INR | Convert US Dollar to Indian Rupee - Western Union',\n",
       "  'url': 'https://www.westernunion.com/us/en/currency-converter/usd-to-inr-rate.html',\n",
       "  'content': '# Convert {sendAmount} US Dollar to Indian Rupee\\n\\nConvert USD to INR now for today’s Western Union exchange rate. Enter the amount, choose your currencies, and see the latest rates. \\u200b\\n\\nFX: 1.00 USD –   Exchange Rates and Fees shown are estimates, vary by a number of factors including payment and payout methods, and are subject to change. To check current rates and other options, simply click “Send money”.\\u200b  X Fee:\\n\\nBe informed. Be aware. Protect yourself from fraud\\n\\nBe informed. Be aware. [...] .828%200-1.41-.499-1.41-1.255V13.393h3.706l-.052-4.595h-3.654V1.972l-5.338%201.337v5.489h-2.786l-.015%204.595h2.801zM266.636%2011.832V8.798h-4.699v23.988h5.648l-.046-14.67c.073-1.476.568-2.607%201.472-3.36%201.186-.99%203.033-1.328%205.486-1.003l.295.039.05-5.006-.23-.031c-3.384-.462-6.446.572-7.976%203.077%22%2F%3E%3Cg%20transform%3D%22translate(137.638%208 [...] .346%22%2F%3E%3C%2Fg%3E%3C%2Fsvg%3E)',\n",
       "  'score': 0.7938904},\n",
       " {'title': 'USD to INR Exchange Rates - Convert US dollars to Indian rupees',\n",
       "  'url': 'https://www.remitly.com/us/en/currency-converter/usd-to-inr-rate',\n",
       "  'content': 'New customer offer\\n\\n# USD to INR\\n\\n## Remitly offers dependable exchange rates for USD to INR with no hidden fees. Join today and get a promotional rate of 88.28 INR to 1 USD on your first money transfer.\\n\\nYou send\\n\\nUSD\\n\\nThey receive\\n\\nINR\\n\\nFee\\n\\n-\\n\\nTotal cost\\n\\n-\\n\\nand more [...] New customers only. One per customer. Limited time offer. Any rates shown are subject to change. Promotional FX rate applies to first 10,000.00 USD sent. If you are sending over 10,000.00 USD, the non-promotional FX rate shown applies when you pay by Bank account. See Terms and Conditions for details. [...] New customers only. One per customer. Limited time offer. Any rates shown are subject to change. Promotional FX rate applies to first 10,000.00 USD sent. If you are sending over 10,000.00 USD, the non-promotional FX rate shown applies when you pay by Bank account. See Terms and Conditions for details.\\n\\n## Send money to India\\n\\nFees for when you send from USD to INR\\n\\n| Send Amount (USD) | Fee |\\n --- |\\n| Below $1,000 | $3.99 |\\n| $1,000 or more | $0 |',\n",
       "  'score': 0.77261686},\n",
       " {'title': 'USD to INR Exchange Rate - Bloomberg.com',\n",
       "  'url': 'https://www.bloomberg.com/quote/USDINR:CUR',\n",
       "  'content': \"For inquiries related to this message please contact our support team and provide the reference ID below.\\n\\nBlock reference ID:f371f0fe-aa9f-11f0-b9a5-93a97907e26c\\n\\nGet the most important global markets news at your fingertips with a Bloomberg.com subscription.\\n\\nSUBSCRIBE NOW [...] # Bloomberg\\n\\nNeed help? Contact us \\n\\n## We've detected unusual activity from your computer network\\n\\nTo continue, please click the box below to let us know you're not a robot.\\n\\n### Why did this happen?\\n\\nPlease make sure your browser supports JavaScript and cookies and that you are not blocking them from loading. For more information you can review our Terms of Service and Cookie Policy.\\n\\n### Need Help?\",\n",
       "  'score': 0.604703}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily.invoke(\"provide the current USD to INR conversion rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8bd2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "### combine all the tools in the list\n",
    "tools=[arxiv,wiki,tavily]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ecf529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### intialize my llm model\n",
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "### bind llm with tools (list of tools)\n",
    "llm_with_tools=llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "315c9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03a1dc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'reasoning_content': 'User asks: \"what is the recent AI news\". Likely they want a summary of recent AI news. We should browse current news. Use tavily_search_results_json for up-to-date info. Let\\'s search.', 'tool_calls': [{'id': 'fc_1f2e74d0-7ade-493a-8050-4e1b9a411181', 'function': {'arguments': '{\"query\":\"recent AI news 2025 October\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 291, 'total_tokens': 368, 'completion_time': 0.075824356, 'prompt_time': 0.015590051, 'queue_time': 0.045156424, 'total_time': 0.091414407}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_77f8660d1d', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--9c510d69-a660-47fa-87b9-588b0035b71e-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'recent AI news 2025 October'}, 'id': 'fc_1f2e74d0-7ade-493a-8050-4e1b9a411181', 'type': 'tool_call'}], usage_metadata={'input_tokens': 291, 'output_tokens': 77, 'total_tokens': 368})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke([HumanMessage(content=f\"what is the recent AI news\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5862fb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'tavily_search_results_json',\n",
       "  'args': {'query': 'recent AI news'},\n",
       "  'id': 'fc_d19738ed-44d6-404d-9026-ebde64e46091',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke([HumanMessage(content=f\"what is the recent AI news\")]).tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5c9cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Entire chatbot with langgraph\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START,END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1023279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "class State(TypedDict):\n",
    "    messages:Annotated[list[AnyMessage],add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a1ec369",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Node definition\n",
    "def tool_calling_llm(state:State):\n",
    "    return {\"messages\":[llm_with_tools.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f944874",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder=StateGraph(State)\n",
    "builder.add_node(\"tool_calling_llm\",tool_calling_llm)\n",
    "builder.add_node(\"tools\",ToolNode(tools))\n",
    "builder.add_edge(START,\"tool_calling_llm\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\",\n",
    "    tools_condition\n",
    ")\n",
    "builder.add_edge(\"tools\",END)\n",
    "graph=builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af543c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint, PrettyPrinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "529e9af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is attention is all you need\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (fc_b46bf656-92b2-4343-a235-3fa3acbd033e)\n",
      " Call ID: fc_b46bf656-92b2-4343-a235-3fa3acbd033e\n",
      "  Args:\n",
      "    query: Attention Is All You Need paper summary\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"“Attention is All You Need” Summary - Medium\", \"url\": \"https://medium.com/@dminhk/attention-is-all-you-need-summary-6f0437e63a91\", \"content\": \"Sitemap\\n\\nOpen in app\\n\\nSign in\\n\\nSign in\\n\\n# “Attention is All You Need” Summary\\n\\nDavid Min\\n\\n3 min readAug 14, 2023\\n\\n> Source: \\n\\n“Attention Is All You Need” is a research paper by Ashish Vaswani et al. that proposes a new neural network architecture for sequence-to-sequence tasks, called the Transformer model. The paper challenges the conventional wisdom that recurrence and convolution are necessary for sequence-to-sequence tasks, and instead advocates for the use of self-attention mechanisms. [...] In summary, “Attention Is All You Need” presents a novel approach to sequence-to-sequence tasks that relies solely on self-attention mechanisms, without using recurrence or convolution. The Transformer model, which incorporates this approach, achieves state-of-the-art results on several machine translation tasks and demonstrates the effectiveness of attention mechanisms for capturing complex relationships in sequential data.\\n\\n# Text Summarization Workflow [...] 1. Introduction: The paper begins by highlighting the limitations of traditional sequence-to-sequence models, which rely on recurrence and convolution to process sequential data. The authors argue that these models are not optimal for tasks that require long-range dependencies and contextual understanding, such as machine translation and text summarization.\", \"score\": 0.9108078}, {\"title\": \"Attention Is All You Need - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\", \"content\": \"\\\"Attention Is All You Need\\\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer \\\"Transformer (machine learning model)\\\"), based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a [...] Since the model relies on Query (Q), Key (K) and Value (V) matrices that come from the same source (i.e. the input sequence or context window), this eliminates the need for RNNs completely ensuring parallelizability for the architecture. This differs from the original form of the Attention mechanism introduced in 2014. Additionally, the paper also discusses the use of an additional scaling factor that was found to be most effective with respect to the dimension of the key vectors (represented [...] > Six of the eight authors were born outside the United States; the other two are children of two green-card-carrying Germans who were temporarily in California and a first-generation American whose family had fled persecution, respectively.\\n\\nAfter the paper, each of the authors left Google to join other companies or to found startups. Several of them expressed feelings of being unable to innovate and expand the Transformer in a direction they want, if they had stayed at Google.\", \"score\": 0.8886914}, {\"title\": \"(PDF) Attention is All you Need (2017) | Ashish Vaswani - SciSpace\", \"url\": \"https://scispace.com/papers/attention-is-all-you-need-1hodz0wcqb\", \"content\": \"Explain Abstract of this paper, Conclusions from the paper, Results of the paper, Methods used in this paper, Summarise introduction of this paper, What are the contributions of this paper, Explain the practical implications of this paper, Limitations of this paper, Literature survey of this paper, What data has been used in this paper, Future works suggested in this paper, Find Related Papers\\n\\n+12 more [...] TL;DR: This paper proposed a simple network architecture based solely on an attention mechanism, dispensing with recurrence and convolutions entirely and achieved state-of-the-art performance on English-to-French translation.\\n\\nread more [...] Abstract: The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being\", \"score\": 0.76951396}, {\"title\": \"I Finally Understood “Attention is All You Need” After So Long ...\", \"url\": \"https://ai.plainenglish.io/i-finally-understood-attention-is-all-you-need-after-so-long-heres-how-i-did-it-263b46273f9f\", \"content\": \"Attention is essentially a way to map a query and a set of key-value pairs to an output. It helps the model “focus” or “pay attention” to a specific part of a sequence. The paper focuses on two key types:\\n\\nWhy the emphasis on self-attention in the paper?\\n\\n1. Faster and more parallelisable.\\n2. Learns long-distance relationships better.\\n3. Easier to visualise and interpret.\\n\\nIn the paper, they also discussed two techniques to calculate attention: [...] The whole point of the paper is this: What if we stopped using complicated stuff like RNNs and CNNs to build sequence transduction models and just used attention instead?\\n To prove this idea, they introduced a new architecture called the Transformer, which was built solely with the attention mechanism, without any recurrence or convolutions.\\n It worked and proved to be more efficient than the State-of-The-Art (SOTA) models at the time, using BLEU scores as the evaluation metric. [...] That’s essentially the summary of the paper. It’s a dense paper, but breaking it down using the 3-pass method made it much more manageable and, dare I say, enjoyable.\\n\\nIf you’ve been putting off reading this paper or a similar one like I did, here’s your sign to try. Use the 3-pass method, and feel free to let me know how it goes.\\n\\nUntil next time!\\n\\n# References\", \"score\": 0.76659125}, {\"title\": \"[1706.03762] Attention Is All You Need - arXiv\", \"url\": \"https://arxiv.org/abs/1706.03762\", \"content\": \"> Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while [...] |  |  |\\n --- |\\n| Comments: | 15 pages, 5 figures |\\n| Subjects: | Computation and Language (cs.CL); Machine Learning (cs.LG) |\\n| Cite as: | arXiv:1706.03762 [cs.CL] |\\n|  | (or  arXiv:1706.03762v7 [cs.CL] for this version) |\\n|  |  arXiv-issued DOI via DataCite |\\n\\n## Submission history [...] We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate\\n\\n> cs > arXiv:1706.03762\\n\\n# Computer Science > Computation and Language\\n\\narXiv:1706.03762 (cs)\\n\\nSubmitted on 12 Jun 2017 ([v1), last revised 2 Aug 2023 (this version, v7)]\\n\\n# Title:Attention Is All You Need\\n\\nAuthors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\", \"score\": 0.7574541}]\n"
     ]
    }
   ],
   "source": [
    "messages=graph.invoke({\"messages\":HumanMessage(content=\"what is attention is all you need\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aed8aab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "1706.03762\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv (fc_d925b6d4-74c7-4b97-bda2-a5e698eba033)\n",
      " Call ID: fc_d925b6d4-74c7-4b97-bda2-a5e698eba033\n",
      "  Args:\n",
      "    query: 1706.03762\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv\n",
      "\n",
      "Published: 2023-08-02\n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "Summary: The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks in an encoder-decoder configuration. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer, base\n"
     ]
    }
   ],
   "source": [
    "messages=graph.invoke({\"messages\":HumanMessage(content=\"1706.03762\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db26a677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "who is barrack obama\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  wikipedia (fc_5a3d0da8-1301-423e-ad8e-d500a26cb687)\n",
      " Call ID: fc_5a3d0da8-1301-423e-ad8e-d500a26cb687\n",
      "  Args:\n",
      "    query: Barack Obama\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: wikipedia\n",
      "\n",
      "Page: Barack Obama\n",
      "Summary: Barack Hussein Obama II (born August 4, 1961) is an American politician who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, he was the first African American president. Obama previously served as a U.S. senator representing Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004.\n",
      "Born in Honolulu, Hawaii, Obama graduated from Columbia University in 1983 with a Bachelor of Arts degree in polit\n"
     ]
    }
   ],
   "source": [
    "messages=graph.invoke({\"messages\":HumanMessage(content=\"who is barrack obama\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ed78a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cuda12_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
