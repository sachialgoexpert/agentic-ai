{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "aa381816",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "11f38a4f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi Sachchida, nice to meet you. That's great to hear that you're an AI enthusiast. Artificial Intelligence is a fascinating field with endless possibilities and applications. What aspects of AI interest you the most? Are you more into machine learning, natural language processing, computer vision, or something else?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 46, 'total_tokens': 108, 'completion_time': 0.165454335, 'prompt_time': 0.002137349, 'queue_time': 0.054589411, 'total_time': 0.167591684}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_34d416ee39', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--3aa14bbe-74f3-4568-b361-bf668930d494-0', usage_metadata={'input_tokens': 46, 'output_tokens': 62, 'total_tokens': 108})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
        "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
        "os.environ[\"TAVILY_API_KEY\"]=os.getenv(\"TAVILY_API_KEY\")\n",
        "from langchain_groq import ChatGroq\n",
        "llm_groq=ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "llm_groq.invoke(\"hi I am sachchida. I am AI enthusiast\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0821e4b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"USER_AGENT\"] = \"LangChainApp/1.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ba1756cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6f827238",
      "metadata": {},
      "outputs": [],
      "source": [
        "urls=[\n",
        "    \"https://academy.langchain.com/courses/intro-to-langgraph\",\n",
        "    \"https://langchain-ai.github.io/langgraph/tutorials/workflows/\",\n",
        "    \"https://langchain-ai.github.io/langgraphjs/how-tos/map-reduce/\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bfcdc1d7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[Document(metadata={'source': 'https://academy.langchain.com/courses/intro-to-langgraph', 'title': 'Foundation: Introduction to LangGraph', 'description': 'Learn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFoundation: Introduction to LangGraph\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\nPython\\n\\nLangChain\\nLangSmith\\nLangGraph\\n\\n\\n\\nJavaScript\\n\\nLangChain\\nLangSmith\\nLangGraph\\n\\n\\n\\n\\nCommunity\\nLangSmith\\nAll Courses\\n\\n\\nSign In\\n\\n\\nRegister\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Academy\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFoundation: Introduction to LangGraph\\nLearn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.\\n\\n\\n\\nEnroll for free\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWatch Intro Video\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCourse Curriculum\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Welcome to the course!\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nCourse Overview\\n\\n\\n\\n\\n\\n\\n\\nGetting Set Up\\n\\n\\n\\n\\n\\n\\n\\nGetting Set Up (Video Guide)\\n\\n\\n\\n\\n\\n\\n\\nModule 0 Resources\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 1: Introduction\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 1 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 1 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Motivation\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: Simple Graph\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: LangSmith Studio\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Chain\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Router\\n\\n\\n\\n\\n\\n\\n\\nLesson 6: Agent\\n\\n\\n\\n\\n\\n\\n\\nLesson 7: Agent with Memory\\n\\n\\n\\n\\n\\n\\n\\n[Optional] Lesson 8: Intro to Deployment\\n\\n\\n\\n\\n\\n\\n\\nModule 1 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 2: State and Memory\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 2 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 2 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: State Schema\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: State Reducers\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Multiple Schemas\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Trim and Filter Messages\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Chatbot w/ Summarizing Messages and Memory\\n\\n\\n\\n\\n\\n\\n\\nLesson 6: Chatbot w/ Summarizing Messages and External Memory\\n\\n\\n\\n\\n\\n\\n\\nModule 2 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 3: UX and Human-in-the-Loop\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 3 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 3 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Streaming\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: Breakpoints\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Editing State and Human Feedback\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Dynamic Breakpoints\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Time Travel\\n\\n\\n\\n\\n\\n\\n\\nModule 3 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 4: Building Your Assistant\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 4 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 4 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Parallelization\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: Sub-graphs\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Map-reduce\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Research Assistant\\n\\n\\n\\n\\n\\n\\n\\nModule 4 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 5: Long-Term Memory\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 5 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Short vs. Long-Term Memory\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: LangGraph Store\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Memory Schema + Profile\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Memory Schema + Collection\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Build an Agent with Long-Term Memory\\n\\n\\n\\n\\n\\n\\n\\nModule 5 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 6: Deployment\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 6 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Deployment Concepts\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: Creating a Deployment\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Connecting to a Deployment\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Double Texting\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Assistants\\n\\n\\n\\n\\n\\n\\n\\nModule 6 Feedback\\n\\n\\n\\n\\n\\n\\n\\nEnd of Course Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Show more\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        About this course\\n      \\n\\n\\n\\n\\nFree\\n\\n\\n\\n\\n            54 lessons\\n          \\n\\n\\n\\n\\n            \\n            6 hours of video content\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph FAQs\\n\\n\\n\\n\\n\\n\\n\\n\\n                      Do I need to use LangChain to use LangGraph? What’s the difference?\\n                    \\n\\nNo. LangGraph is an orchestration framework for complex agentic systems and is more low-level and controllable than LangChain agents. On the other hand, LangChain provides a standard interface to interact with models and other components, useful for straight-forward chains and retrieval flows.\\n\\n\\n\\n\\n\\n\\n\\n                      How is LangGraph different from other agent frameworks?\\n                    \\n\\nOther agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a company’s needs. LangGraph provides a more expressive framework to handle companies’ unique tasks without restricting users to a single black-box cognitive architecture.\\n\\n\\n\\n\\n\\n\\n\\n                      Does LangGraph impact the performance of my app?\\n                    \\n\\nLangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.\\n\\n\\n\\n\\n\\n\\n\\n                      Is LangGraph open source? Is it free?\\n                    \\n\\nYes. LangGraph is an MIT-licensed open-source library and is free to use.\\n\\n\\n\\n\\n\\n\\n\\n                      What is LangSmith Deployment?\\n                    \\n\\nLangSmith Deployment helps you ship your agent in one click, using scalable infrastructure built for long-running tasks.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReady to start shipping reliable agents faster?\\nOur platform provides tools for every step of the agent development lifecycle — built to unlock powerful AI in production.\\n\\n\\n\\n                Contact Sales\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLearn with the community\\nLearn alongside other builders at in-person LangChain meetups. Subscribe to our calendar to be the first to know about upcoming events near you.\\n\\n\\n\\n                Subscribe\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n            © Copyright LangChain Academy 2025\\n          \\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')],\n",
              " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Workflows & agents', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows & agents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nThese docs will be deprecated and removed with the release of LangGraph v1.0 in October 2025. Visit the v1.0 alpha docs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Workflows & agents\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n    Build a custom workflow\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Run a local server\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    General concepts\\n    \\n  \\n\\n\\n\\n\\n\\n            General concepts\\n          \\n\\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows and Agents¶\\nThis guide reviews common patterns for agentic systems. In describing these systems, it can be useful to make a distinction between \"workflows\" and \"agents\". One way to think about this difference is nicely explained in Anthropic\\'s Building Effective Agents blog post:\\n\\nWorkflows are systems where LLMs and tools are orchestrated through predefined code paths.\\nAgents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\\n\\nHere is a simple way to visualize these differences:\\n\\nWhen building agents and workflows, LangGraph offers a number of benefits including persistence, streaming, and support for debugging as well as deployment.\\nSet up¶\\nYou can use any chat model that supports structured outputs and tool calling. Below, we show the process of installing the packages, setting API keys, and testing structured outputs / tool calling for Anthropic.\\n\\nInstall dependencies\\npip install langchain_core langchain-anthropic langgraph\\n\\n\\nInitialize an LLM\\nAPI Reference: ChatAnthropic\\nimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\\n\\nBuilding Blocks: The Augmented LLM¶\\nLLM have augmentations that support building workflows and agents. These include structured outputs and tool calling, as shown in this image from the Anthropic blog on Building Effective Agents:\\n\\n# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\nPrompt chaining¶\\nIn prompt chaining, each LLM call processes the output of the previous one.\\nAs noted in the Anthropic blog on Building Effective Agents:\\n\\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate\" in the diagram below) on any intermediate steps to ensure that the process is still on track.\\nWhen to use this workflow: This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Joke failed quality gate - no punchline detected!\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/a0281fca-3a71-46de-beee-791468607b75/r\\nResources:\\nLangChain Academy\\nSee our lesson on Prompt Chaining here.\\n\\n\\nfrom langgraph.func import entrypoint, task\\n\\n\\n# Tasks\\n@task\\ndef generate_joke(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a short joke about {topic}\")\\n    return msg.content\\n\\n\\ndef check_punchline(joke: str):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in joke or \"!\" in joke:\\n        return \"Fail\"\\n\\n    return \"Pass\"\\n\\n\\n@task\\ndef improve_joke(joke: str):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\\n    return msg.content\\n\\n\\n@task\\ndef polish_joke(joke: str):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\\n    return msg.content\\n\\n\\n@entrypoint()\\ndef prompt_chaining_workflow(topic: str):\\n    original_joke = generate_joke(topic).result()\\n    if check_punchline(original_joke) == \"Pass\":\\n        return original_joke\\n\\n    improved_joke = improve_joke(original_joke).result()\\n    return polish_joke(improved_joke).result()\\n\\n# Invoke\\nfor step in prompt_chaining_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/332fa4fc-b6ca-416e-baa3-161625e69163/r\\n\\n\\n\\nParallelization¶\\nWith parallelization, LLMs work simultaneously on a task:\\n\\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations: Sectioning: Breaking a task into independent subtasks run in parallel. Voting: Running the same task multiple times to get diverse outputs.\\nWhen to use this workflow: Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/3be2e53c-ca94-40dd-934f-82ff87fac277/r\\nResources:\\nDocumentation\\nSee our documentation on parallelization here.\\nLangChain Academy\\nSee our lesson on parallelization here.\\n\\n\\n@task\\ndef call_llm_1(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_2(topic: str):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n    msg = llm.invoke(f\"Write a story about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_3(topic):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n    msg = llm.invoke(f\"Write a poem about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef aggregator(topic, joke, story, poem):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {topic}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{story}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{joke}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{poem}\"\\n    return combined\\n\\n\\n# Build workflow\\n@entrypoint()\\ndef parallel_workflow(topic: str):\\n    joke_fut = call_llm_1(topic)\\n    story_fut = call_llm_2(topic)\\n    poem_fut = call_llm_3(topic)\\n    return aggregator(\\n        topic, joke_fut.result(), story_fut.result(), poem_fut.result()\\n    ).result()\\n\\n# Invoke\\nfor step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/623d033f-e814-41e9-80b1-75e6abb67801/r\\n\\n\\n\\nRouting¶\\nRouting classifies an input and directs it to a followup task. As noted in the Anthropic blog on Building Effective Agents:\\n\\nRouting classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.\\nWhen to use this workflow: Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import Literal\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\\n\\n# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/c4580b74-fe91-47e4-96fe-7fac598d509c/r\\nResources:\\nLangChain Academy\\nSee our lesson on routing here.\\nExamples\\nHere is RAG workflow that routes questions. See our video here.\\n\\n\\nfrom typing_extensions import Literal\\nfrom pydantic import BaseModel\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n@task\\ndef llm_call_1(input_: str):\\n    \"\"\"Write a story\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_2(input_: str):\\n    \"\"\"Write a joke\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_3(input_: str):\\n    \"\"\"Write a poem\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\ndef llm_call_router(input_: str):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=input_),\\n        ]\\n    )\\n    return decision.step\\n\\n\\n# Create workflow\\n@entrypoint()\\ndef router_workflow(input_: str):\\n    next_step = llm_call_router(input_)\\n    if next_step == \"story\":\\n        llm_call = llm_call_1\\n    elif next_step == \"joke\":\\n        llm_call = llm_call_2\\n    elif next_step == \"poem\":\\n        llm_call = llm_call_3\\n\\n    return llm_call(input_).result()\\n\\n# Invoke\\nfor step in router_workflow.stream(\"Write me a joke about cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/5e2eb979-82dd-402c-b1a0-a8cceaf2a28a/r\\n\\n\\n\\nOrchestrator-Worker¶\\nWith orchestrator-worker, an orchestrator breaks down a task and delegates each sub-task to workers. As noted in the Anthropic blog on Building Effective Agents:\\n\\nIn the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\\nWhen to use this workflow: This workflow is well-suited for complex tasks where you can\\'t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it\\'s topographically similar, the key difference from parallelization is its flexibility—subtasks aren\\'t pre-defined, but determined by the orchestrator based on the specific input.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\nCreating Workers in LangGraph\\nBecause orchestrator-worker workflows are common, LangGraph has the Send API to support this. It lets you dynamically create worker nodes and send each one a specific input. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. As you can see below, we iterate over a list of sections and Send each to a worker node. See further documentation here and here.\\nfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report\\n\\n\\n# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n\\n    return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\\n\\n# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/78cbcfc3-38bf-471d-b62a-b299b144237d/r\\nResources:\\nLangChain Academy\\nSee our lesson on orchestrator-worker here.\\nExamples\\nHere is a project that uses orchestrator-worker for report planning and writing. See our video here.\\n\\n\\nfrom typing import List\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\n@task\\ndef orchestrator(topic: str):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {topic}\"),\\n        ]\\n    )\\n\\n    return report_sections.sections\\n\\n\\n@task\\ndef llm_call(section: Section):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    result = llm.invoke(\\n        [\\n            SystemMessage(content=\"Write a report section.\"),\\n            HumanMessage(\\n                content=f\"Here is the section name: {section.name} and description: {section.description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return result.content\\n\\n\\n@task\\ndef synthesizer(completed_sections: list[str]):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n    final_report = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n    return final_report\\n\\n\\n@entrypoint()\\ndef orchestrator_worker(topic: str):\\n    sections = orchestrator(topic).result()\\n    section_futures = [llm_call(section) for section in sections]\\n    final_report = synthesizer(\\n        [section_fut.result() for section_fut in section_futures]\\n    ).result()\\n    return final_report\\n\\n# Invoke\\nreport = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\\nfrom IPython.display import Markdown\\nMarkdown(report)\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/75a636d0-6179-4a12-9836-e0aa571e87c5/r\\n\\n\\n\\nEvaluator-optimizer¶\\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop:\\n\\nWhen to use this workflow: This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\\n\\n\\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)\\n\\n# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/86ab3e60-2000-4bff-b988-9b89a3269789/r\\nResources:\\nExamples\\nHere is an assistant that uses evaluator-optimizer to improve a report. See our video here.\\nHere is a RAG workflow that grades answers for hallucinations or errors. See our video here.\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\n@task\\ndef llm_call_generator(topic: str, feedback: Feedback):\\n    \"\"\"LLM generates a joke\"\"\"\\n    if feedback:\\n        msg = llm.invoke(\\n            f\"Write a joke about {topic} but take into account the feedback: {feedback}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef llm_call_evaluator(joke: str):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n    feedback = evaluator.invoke(f\"Grade the joke {joke}\")\\n    return feedback\\n\\n\\n@entrypoint()\\ndef optimizer_workflow(topic: str):\\n    feedback = None\\n    while True:\\n        joke = llm_call_generator(topic, feedback).result()\\n        feedback = llm_call_evaluator(joke).result()\\n        if feedback.grade == \"funny\":\\n            break\\n\\n    return joke\\n\\n# Invoke\\nfor step in optimizer_workflow.stream(\"Cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/f66830be-4339-4a6b-8a93-389ce5ae27b4/r\\n\\n\\n\\nAgent¶\\nAgents are typically implemented as an LLM performing actions (via tool-calling) based on environmental feedback in a loop. As noted in the Anthropic blog on Building Effective Agents:\\n\\nAgents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully.\\nWhen to use agents: Agents can be used for open-ended problems where it\\'s difficult or impossible to predict the required number of steps, and where you can\\'t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents\\' autonomy makes them ideal for scaling tasks in trusted environments.\\n\\n\\nAPI Reference: tool\\nfrom langchain_core.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional API\\n\\n\\nfrom langgraph.graph import MessagesState\\nfrom langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\n\\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"Action\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"Action\"\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"environment\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    {\\n        # Name returned by should_continue : Name of next node to visit\\n        \"Action\": \"environment\",\\n        END: END,\\n    },\\n)\\nagent_builder.add_edge(\"environment\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/051f0391-6761-4f8c-a53b-22231b016690/r\\nResources:\\nLangChain Academy\\nSee our lesson on agents here.\\nExamples\\nHere is a project that uses a tool calling agent to create / store long-term memories.\\n\\n\\nfrom langgraph.graph import add_messages\\nfrom langchain_core.messages import (\\n    SystemMessage,\\n    HumanMessage,\\n    BaseMessage,\\n    ToolCall,\\n)\\n\\n\\n@task\\ndef call_llm(messages: list[BaseMessage]):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n    return llm_with_tools.invoke(\\n        [\\n            SystemMessage(\\n                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n            )\\n        ]\\n        + messages\\n    )\\n\\n\\n@task\\ndef call_tool(tool_call: ToolCall):\\n    \"\"\"Performs the tool call\"\"\"\\n    tool = tools_by_name[tool_call[\"name\"]]\\n    return tool.invoke(tool_call)\\n\\n\\n@entrypoint()\\ndef agent(messages: list[BaseMessage]):\\n    llm_response = call_llm(messages).result()\\n\\n    while True:\\n        if not llm_response.tool_calls:\\n            break\\n\\n        # Execute tools\\n        tool_result_futures = [\\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\\n        ]\\n        tool_results = [fut.result() for fut in tool_result_futures]\\n        messages = add_messages(messages, [llm_response, *tool_results])\\n        llm_response = call_llm(messages).result()\\n\\n    messages = add_messages(messages, llm_response)\\n    return messages\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nfor chunk in agent.stream(messages, stream_mode=\"updates\"):\\n    print(chunk)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/42ae8bf9-3935-4504-a081-8ddbcbfc8b2e/r\\n\\n\\n\\nPre-built¶\\nLangGraph also provides a pre-built method for creating an agent as defined above (using the create_react_agent function):\\nhttps://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\\nAPI Reference: create_react_agent\\nfrom langgraph.prebuilt import create_react_agent\\n\\n# Pass in:\\n# (1) the augmented LLM with tools\\n# (2) the tools list (which is used to create the tool node)\\npre_built_agent = create_react_agent(llm, tools=tools)\\n\\n# Show the agent\\ndisplay(Image(pre_built_agent.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = pre_built_agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/abab6a44-29f6-4b97-8164-af77413e494d/r\\nWhat LangGraph provides¶\\nBy constructing each of the above in LangGraph, we get a few things:\\nPersistence: Human-in-the-Loop¶\\nLangGraph persistence layer supports interruption and approval of actions (e.g., Human In The Loop). See Module 3 of LangChain Academy.\\nPersistence: Memory¶\\nLangGraph persistence layer supports conversational (short-term) memory and long-term memory. See Modules 2 and 5 of LangChain Academy:\\nStreaming¶\\nLangGraph provides several ways to stream workflow / agent outputs or intermediate state. See Module 3 of LangChain Academy.\\nDeployment¶\\nLangGraph provides an easy on-ramp for deployment, observability, and evaluation. See module 6 of LangChain Academy.\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Run a local server\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Agent architectures\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')],\n",
              " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraphjs/how-tos/map-reduce/', 'title': 'How to create map-reduce branches for parallel execution', 'description': 'Build language agents as graphs', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow to create map-reduce branches for parallel execution\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nThese docs will be deprecated and removed with the release of LangGraph v1.0 in October 2025. Visit the v1.0 alpha docs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n              How to create map-reduce branches for parallel execution\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n    \\n  \\n  LangGraph\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Agents\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  API reference\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Versions\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n  \\n    LangGraph\\n  \\n\\n          \\n\\n\\n\\n\\n\\n    \\n  \\n    Get started\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Get started\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    Learn the basics\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Deployment\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Guides\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Guides\\n  \\n\\n          \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    How-to Guides\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n  \\n    How-to Guides\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    Installation\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    LangGraph\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    LangGraph\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Controllability\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Controllability\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    Controllability\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    How to create map-reduce branches for parallel execution\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n    \\n  \\n    How to create map-reduce branches for parallel execution\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      \\n        Setup\\n      \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    How to create branches for parallel node execution\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    How to combine control flow and state updates with Command\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    How to create and control loops\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    How to defer node execution\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Persistence\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Memory\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Human-in-the-loop\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Streaming\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Tool calling\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Subgraphs\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Multi-agent\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    State Management\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Other\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Prebuilt ReAct Agent\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph Platform\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Concepts\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Tutorials\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Resources\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Resources\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    Adopters\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    LLMS-txt\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    FAQ\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Troubleshooting\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph Academy Course\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Agents\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    API reference\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Versions\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      \\n        Setup\\n      \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n    Guides\\n  \\n\\n\\n\\n\\n\\n    How-to Guides\\n  \\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n    Controllability\\n  \\n\\n\\n\\n\\n\\n\\n\\nHow to create map-reduce branches for parallel execution¶\\nMap-reduce operations are essential for efficient task decomposition and parallel processing. This approach involves breaking a task into smaller sub-tasks, processing each sub-task in parallel, and aggregating the results across all of the completed sub-tasks. \\nConsider this example: given a general topic from the user, generate a list of related subjects, generate a joke for each subject, and select the best joke from the resulting list. In this design pattern, a first node may generate a list of objects (e.g., related subjects) and we want to apply some other node (e.g., generate a joke) to all those objects (e.g., subjects). However, two main challenges arise.\\n(1) the number of objects (e.g., subjects) may be unknown ahead of time (meaning the number of edges may not be known) when we lay out the graph and (2) the input State to the downstream Node should be different (one for each generated object).\\nLangGraph addresses these challenges through its Send API. By utilizing conditional edges, Send can distribute different states (e.g., subjects) to multiple instances of a node (e.g., joke generation). Importantly, the sent state can differ from the core graph\\'s state, allowing for flexible and dynamic workflow management. \\n\\nSetup¶\\nThis example will require a few dependencies. First, install the LangGraph library, along with the @langchain/anthropic package as we\\'ll be using Anthropic LLMs in this example:\\nnpm install @langchain/langgraph @langchain/anthropic @langchain/core\\n\\nNext, set your Anthropic API key:\\nprocess.env.ANTHROPIC_API_KEY = \\'YOUR_API_KEY\\'\\n\\nimport { z } from \"zod\";\\nimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport { StateGraph, END, START, Annotation, Send } from \"@langchain/langgraph\";\\n\\n/* Model and prompts */\\n\\n// Define model and prompts we will use\\nconst subjectsPrompt = \"Generate a comma separated list of between 2 and 5 examples related to: {topic}.\"\\nconst jokePrompt = \"Generate a joke about {subject}\"\\nconst bestJokePrompt = `Below are a bunch of jokes about {topic}. Select the best one! Return the ID (index) of the best one.\\n\\n{jokes}`\\n\\n// Zod schemas for getting structured output from the LLM\\nconst Subjects = z.object({\\n  subjects: z.array(z.string()),\\n});\\nconst Joke = z.object({\\n  joke: z.string(),\\n});\\nconst BestJoke = z.object({\\n  id: z.number(),\\n});\\n\\nconst model = new ChatAnthropic({\\n  model: \"claude-3-5-sonnet-20240620\",\\n});\\n\\n/* Graph components: define the components that will make up the graph */\\n\\n// This will be the overall state of the main graph.\\n// It will contain a topic (which we expect the user to provide)\\n// and then will generate a list of subjects, and then a joke for\\n// each subject\\nconst OverallState = Annotation.Root({\\n  topic: Annotation<string>,\\n  subjects: Annotation<string[]>,\\n  // Notice here we pass a reducer function.\\n  // This is because we want combine all the jokes we generate\\n  // from individual nodes back into one list.\\n  jokes: Annotation<string[]>({\\n    reducer: (state, update) => state.concat(update),\\n  }),\\n  bestSelectedJoke: Annotation<string>,\\n});\\n\\n// This will be the state of the node that we will \"map\" all\\n// subjects to in order to generate a joke\\ninterface JokeState {\\n  subject: string;\\n}\\n\\n// This is the function we will use to generate the subjects of the jokes\\nconst generateTopics = async (\\n  state: typeof OverallState.State\\n): Promise<Partial<typeof OverallState.State>> => {\\n  const prompt = subjectsPrompt.replace(\"topic\", state.topic);\\n  const response = await model\\n    .withStructuredOutput(Subjects, { name: \"subjects\" })\\n    .invoke(prompt);\\n  return { subjects: response.subjects };\\n};\\n\\n// Function to generate a joke\\nconst generateJoke = async (state: JokeState): Promise<{ jokes: string[] }> => {\\n  const prompt = jokePrompt.replace(\"subject\", state.subject);\\n  const response = await model\\n    .withStructuredOutput(Joke, { name: \"joke\" })\\n    .invoke(prompt);\\n  return { jokes: [response.joke] };\\n};\\n\\n// Here we define the logic to map out over the generated subjects\\n// We will use this an edge in the graph\\nconst continueToJokes = (state: typeof OverallState.State) => {\\n  // We will return a list of `Send` objects\\n  // Each `Send` object consists of the name of a node in the graph\\n  // as well as the state to send to that node\\n  return state.subjects.map((subject) => new Send(\"generateJoke\", { subject }));\\n};\\n\\n// Here we will judge the best joke\\nconst bestJoke = async (\\n  state: typeof OverallState.State\\n): Promise<Partial<typeof OverallState.State>> => {\\n  const jokes = state.jokes.join(\"\\\\n\\\\n\");\\n  const prompt = bestJokePrompt\\n    .replace(\"jokes\", jokes)\\n    .replace(\"topic\", state.topic);\\n  const response = await model\\n    .withStructuredOutput(BestJoke, { name: \"best_joke\" })\\n    .invoke(prompt);\\n  return { bestSelectedJoke: state.jokes[response.id] };\\n};\\n\\n// Construct the graph: here we put everything together to construct our graph\\nconst graph = new StateGraph(OverallState)\\n  .addNode(\"generateTopics\", generateTopics)\\n  .addNode(\"generateJoke\", generateJoke)\\n  .addNode(\"bestJoke\", bestJoke)\\n  .addEdge(START, \"generateTopics\")\\n  .addConditionalEdges(\"generateTopics\", continueToJokes)\\n  .addEdge(\"generateJoke\", \"bestJoke\")\\n  .addEdge(\"bestJoke\", END);\\n\\nconst app = graph.compile();\\n\\nimport * as tslab from \"tslab\";\\n\\nconst representation = app.getGraph();\\nconst image = await representation.drawMermaidPng();\\nconst arrayBuffer = await image.arrayBuffer();\\n\\ntslab.display.png(new Uint8Array(arrayBuffer));\\n\\n\\n// Call the graph: here we call it to generate a list of jokes\\nfor await (const s of await app.stream({ topic: \"animals\" })) {\\n  console.log(s);\\n}\\n\\n{\\n  generateTopics: { subjects: [ \\'lion\\', \\'elephant\\', \\'penguin\\', \\'dolphin\\' ] }\\n}\\n{\\n  generateJoke: {\\n    jokes: [ \"Why don\\'t lions like fast food? Because they can\\'t catch it!\" ]\\n  }\\n}\\n{\\n  generateJoke: {\\n    jokes: [\\n      \"Why don\\'t elephants use computers? Because they\\'re afraid of the mouse!\"\\n    ]\\n  }\\n}\\n{\\n  generateJoke: {\\n    jokes: [\\n      \"Why don\\'t dolphins use smartphones? They\\'re afraid of phishing!\"\\n    ]\\n  }\\n}\\n{\\n  generateJoke: {\\n    jokes: [\\n      \"Why don\\'t you see penguins in Britain? Because they\\'re afraid of Wales!\"\\n    ]\\n  }\\n}\\n{\\n  bestJoke: {\\n    bestSelectedJoke: \"Why don\\'t elephants use computers? Because they\\'re afraid of the mouse!\"\\n  }\\n}\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                How to use LangGraph.js in web environments\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                How to create branches for parallel node execution\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs Insiders\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCookie consent\\nWe use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they\\'re searching for. Clicking \"Accept\" makes our documentation better. Thank you! ❤️\\n\\n\\n\\n\\n\\n\\n\\n          GitHub\\n        \\n\\n\\n\\n\\nAccept\\nReject\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs=[WebBaseLoader(url).load() for url in urls]\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a08945f3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(metadata={'source': 'https://academy.langchain.com/courses/intro-to-langgraph', 'title': 'Foundation: Introduction to LangGraph', 'description': 'Learn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFoundation: Introduction to LangGraph\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\nPython\\n\\nLangChain\\nLangSmith\\nLangGraph\\n\\n\\n\\nJavaScript\\n\\nLangChain\\nLangSmith\\nLangGraph\\n\\n\\n\\n\\nCommunity\\nLangSmith\\nAll Courses\\n\\n\\nSign In\\n\\n\\nRegister\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Academy\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFoundation: Introduction to LangGraph\\nLearn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.\\n\\n\\n\\nEnroll for free\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWatch Intro Video\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCourse Curriculum\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Welcome to the course!\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nCourse Overview\\n\\n\\n\\n\\n\\n\\n\\nGetting Set Up\\n\\n\\n\\n\\n\\n\\n\\nGetting Set Up (Video Guide)\\n\\n\\n\\n\\n\\n\\n\\nModule 0 Resources\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 1: Introduction\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 1 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 1 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Motivation\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: Simple Graph\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: LangSmith Studio\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Chain\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Router\\n\\n\\n\\n\\n\\n\\n\\nLesson 6: Agent\\n\\n\\n\\n\\n\\n\\n\\nLesson 7: Agent with Memory\\n\\n\\n\\n\\n\\n\\n\\n[Optional] Lesson 8: Intro to Deployment\\n\\n\\n\\n\\n\\n\\n\\nModule 1 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 2: State and Memory\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 2 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 2 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: State Schema\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: State Reducers\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Multiple Schemas\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Trim and Filter Messages\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Chatbot w/ Summarizing Messages and Memory\\n\\n\\n\\n\\n\\n\\n\\nLesson 6: Chatbot w/ Summarizing Messages and External Memory\\n\\n\\n\\n\\n\\n\\n\\nModule 2 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 3: UX and Human-in-the-Loop\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 3 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 3 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Streaming\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: Breakpoints\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Editing State and Human Feedback\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Dynamic Breakpoints\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Time Travel\\n\\n\\n\\n\\n\\n\\n\\nModule 3 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 4: Building Your Assistant\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 4 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 4 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Parallelization\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: Sub-graphs\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Map-reduce\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Research Assistant\\n\\n\\n\\n\\n\\n\\n\\nModule 4 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 5: Long-Term Memory\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 5 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Short vs. Long-Term Memory\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: LangGraph Store\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Memory Schema + Profile\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Memory Schema + Collection\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Build an Agent with Long-Term Memory\\n\\n\\n\\n\\n\\n\\n\\nModule 5 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 6: Deployment\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 6 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Deployment Concepts\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: Creating a Deployment\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Connecting to a Deployment\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Double Texting\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Assistants\\n\\n\\n\\n\\n\\n\\n\\nModule 6 Feedback\\n\\n\\n\\n\\n\\n\\n\\nEnd of Course Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Show more\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        About this course\\n      \\n\\n\\n\\n\\nFree\\n\\n\\n\\n\\n            54 lessons\\n          \\n\\n\\n\\n\\n            \\n            6 hours of video content\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph FAQs\\n\\n\\n\\n\\n\\n\\n\\n\\n                      Do I need to use LangChain to use LangGraph? What’s the difference?\\n                    \\n\\nNo. LangGraph is an orchestration framework for complex agentic systems and is more low-level and controllable than LangChain agents. On the other hand, LangChain provides a standard interface to interact with models and other components, useful for straight-forward chains and retrieval flows.\\n\\n\\n\\n\\n\\n\\n\\n                      How is LangGraph different from other agent frameworks?\\n                    \\n\\nOther agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a company’s needs. LangGraph provides a more expressive framework to handle companies’ unique tasks without restricting users to a single black-box cognitive architecture.\\n\\n\\n\\n\\n\\n\\n\\n                      Does LangGraph impact the performance of my app?\\n                    \\n\\nLangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.\\n\\n\\n\\n\\n\\n\\n\\n                      Is LangGraph open source? Is it free?\\n                    \\n\\nYes. LangGraph is an MIT-licensed open-source library and is free to use.\\n\\n\\n\\n\\n\\n\\n\\n                      What is LangSmith Deployment?\\n                    \\n\\nLangSmith Deployment helps you ship your agent in one click, using scalable infrastructure built for long-running tasks.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReady to start shipping reliable agents faster?\\nOur platform provides tools for every step of the agent development lifecycle — built to unlock powerful AI in production.\\n\\n\\n\\n                Contact Sales\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLearn with the community\\nLearn alongside other builders at in-person LangChain meetups. Subscribe to our calendar to be the first to know about upcoming events near you.\\n\\n\\n\\n                Subscribe\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n            © Copyright LangChain Academy 2025\\n          \\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'), Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Workflows & agents', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows & agents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nThese docs will be deprecated and removed with the release of LangGraph v1.0 in October 2025. Visit the v1.0 alpha docs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Workflows & agents\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n    Build a custom workflow\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Run a local server\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    General concepts\\n    \\n  \\n\\n\\n\\n\\n\\n            General concepts\\n          \\n\\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows and Agents¶\\nThis guide reviews common patterns for agentic systems. In describing these systems, it can be useful to make a distinction between \"workflows\" and \"agents\". One way to think about this difference is nicely explained in Anthropic\\'s Building Effective Agents blog post:\\n\\nWorkflows are systems where LLMs and tools are orchestrated through predefined code paths.\\nAgents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\\n\\nHere is a simple way to visualize these differences:\\n\\nWhen building agents and workflows, LangGraph offers a number of benefits including persistence, streaming, and support for debugging as well as deployment.\\nSet up¶\\nYou can use any chat model that supports structured outputs and tool calling. Below, we show the process of installing the packages, setting API keys, and testing structured outputs / tool calling for Anthropic.\\n\\nInstall dependencies\\npip install langchain_core langchain-anthropic langgraph\\n\\n\\nInitialize an LLM\\nAPI Reference: ChatAnthropic\\nimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\\n\\nBuilding Blocks: The Augmented LLM¶\\nLLM have augmentations that support building workflows and agents. These include structured outputs and tool calling, as shown in this image from the Anthropic blog on Building Effective Agents:\\n\\n# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\nPrompt chaining¶\\nIn prompt chaining, each LLM call processes the output of the previous one.\\nAs noted in the Anthropic blog on Building Effective Agents:\\n\\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate\" in the diagram below) on any intermediate steps to ensure that the process is still on track.\\nWhen to use this workflow: This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Joke failed quality gate - no punchline detected!\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/a0281fca-3a71-46de-beee-791468607b75/r\\nResources:\\nLangChain Academy\\nSee our lesson on Prompt Chaining here.\\n\\n\\nfrom langgraph.func import entrypoint, task\\n\\n\\n# Tasks\\n@task\\ndef generate_joke(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a short joke about {topic}\")\\n    return msg.content\\n\\n\\ndef check_punchline(joke: str):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in joke or \"!\" in joke:\\n        return \"Fail\"\\n\\n    return \"Pass\"\\n\\n\\n@task\\ndef improve_joke(joke: str):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\\n    return msg.content\\n\\n\\n@task\\ndef polish_joke(joke: str):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\\n    return msg.content\\n\\n\\n@entrypoint()\\ndef prompt_chaining_workflow(topic: str):\\n    original_joke = generate_joke(topic).result()\\n    if check_punchline(original_joke) == \"Pass\":\\n        return original_joke\\n\\n    improved_joke = improve_joke(original_joke).result()\\n    return polish_joke(improved_joke).result()\\n\\n# Invoke\\nfor step in prompt_chaining_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/332fa4fc-b6ca-416e-baa3-161625e69163/r\\n\\n\\n\\nParallelization¶\\nWith parallelization, LLMs work simultaneously on a task:\\n\\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations: Sectioning: Breaking a task into independent subtasks run in parallel. Voting: Running the same task multiple times to get diverse outputs.\\nWhen to use this workflow: Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/3be2e53c-ca94-40dd-934f-82ff87fac277/r\\nResources:\\nDocumentation\\nSee our documentation on parallelization here.\\nLangChain Academy\\nSee our lesson on parallelization here.\\n\\n\\n@task\\ndef call_llm_1(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_2(topic: str):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n    msg = llm.invoke(f\"Write a story about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_3(topic):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n    msg = llm.invoke(f\"Write a poem about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef aggregator(topic, joke, story, poem):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {topic}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{story}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{joke}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{poem}\"\\n    return combined\\n\\n\\n# Build workflow\\n@entrypoint()\\ndef parallel_workflow(topic: str):\\n    joke_fut = call_llm_1(topic)\\n    story_fut = call_llm_2(topic)\\n    poem_fut = call_llm_3(topic)\\n    return aggregator(\\n        topic, joke_fut.result(), story_fut.result(), poem_fut.result()\\n    ).result()\\n\\n# Invoke\\nfor step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/623d033f-e814-41e9-80b1-75e6abb67801/r\\n\\n\\n\\nRouting¶\\nRouting classifies an input and directs it to a followup task. As noted in the Anthropic blog on Building Effective Agents:\\n\\nRouting classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.\\nWhen to use this workflow: Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import Literal\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\\n\\n# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/c4580b74-fe91-47e4-96fe-7fac598d509c/r\\nResources:\\nLangChain Academy\\nSee our lesson on routing here.\\nExamples\\nHere is RAG workflow that routes questions. See our video here.\\n\\n\\nfrom typing_extensions import Literal\\nfrom pydantic import BaseModel\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n@task\\ndef llm_call_1(input_: str):\\n    \"\"\"Write a story\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_2(input_: str):\\n    \"\"\"Write a joke\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_3(input_: str):\\n    \"\"\"Write a poem\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\ndef llm_call_router(input_: str):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=input_),\\n        ]\\n    )\\n    return decision.step\\n\\n\\n# Create workflow\\n@entrypoint()\\ndef router_workflow(input_: str):\\n    next_step = llm_call_router(input_)\\n    if next_step == \"story\":\\n        llm_call = llm_call_1\\n    elif next_step == \"joke\":\\n        llm_call = llm_call_2\\n    elif next_step == \"poem\":\\n        llm_call = llm_call_3\\n\\n    return llm_call(input_).result()\\n\\n# Invoke\\nfor step in router_workflow.stream(\"Write me a joke about cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/5e2eb979-82dd-402c-b1a0-a8cceaf2a28a/r\\n\\n\\n\\nOrchestrator-Worker¶\\nWith orchestrator-worker, an orchestrator breaks down a task and delegates each sub-task to workers. As noted in the Anthropic blog on Building Effective Agents:\\n\\nIn the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\\nWhen to use this workflow: This workflow is well-suited for complex tasks where you can\\'t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it\\'s topographically similar, the key difference from parallelization is its flexibility—subtasks aren\\'t pre-defined, but determined by the orchestrator based on the specific input.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\nCreating Workers in LangGraph\\nBecause orchestrator-worker workflows are common, LangGraph has the Send API to support this. It lets you dynamically create worker nodes and send each one a specific input. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. As you can see below, we iterate over a list of sections and Send each to a worker node. See further documentation here and here.\\nfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report\\n\\n\\n# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n\\n    return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\\n\\n# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/78cbcfc3-38bf-471d-b62a-b299b144237d/r\\nResources:\\nLangChain Academy\\nSee our lesson on orchestrator-worker here.\\nExamples\\nHere is a project that uses orchestrator-worker for report planning and writing. See our video here.\\n\\n\\nfrom typing import List\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\n@task\\ndef orchestrator(topic: str):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {topic}\"),\\n        ]\\n    )\\n\\n    return report_sections.sections\\n\\n\\n@task\\ndef llm_call(section: Section):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    result = llm.invoke(\\n        [\\n            SystemMessage(content=\"Write a report section.\"),\\n            HumanMessage(\\n                content=f\"Here is the section name: {section.name} and description: {section.description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return result.content\\n\\n\\n@task\\ndef synthesizer(completed_sections: list[str]):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n    final_report = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n    return final_report\\n\\n\\n@entrypoint()\\ndef orchestrator_worker(topic: str):\\n    sections = orchestrator(topic).result()\\n    section_futures = [llm_call(section) for section in sections]\\n    final_report = synthesizer(\\n        [section_fut.result() for section_fut in section_futures]\\n    ).result()\\n    return final_report\\n\\n# Invoke\\nreport = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\\nfrom IPython.display import Markdown\\nMarkdown(report)\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/75a636d0-6179-4a12-9836-e0aa571e87c5/r\\n\\n\\n\\nEvaluator-optimizer¶\\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop:\\n\\nWhen to use this workflow: This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\\n\\n\\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)\\n\\n# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/86ab3e60-2000-4bff-b988-9b89a3269789/r\\nResources:\\nExamples\\nHere is an assistant that uses evaluator-optimizer to improve a report. See our video here.\\nHere is a RAG workflow that grades answers for hallucinations or errors. See our video here.\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\n@task\\ndef llm_call_generator(topic: str, feedback: Feedback):\\n    \"\"\"LLM generates a joke\"\"\"\\n    if feedback:\\n        msg = llm.invoke(\\n            f\"Write a joke about {topic} but take into account the feedback: {feedback}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef llm_call_evaluator(joke: str):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n    feedback = evaluator.invoke(f\"Grade the joke {joke}\")\\n    return feedback\\n\\n\\n@entrypoint()\\ndef optimizer_workflow(topic: str):\\n    feedback = None\\n    while True:\\n        joke = llm_call_generator(topic, feedback).result()\\n        feedback = llm_call_evaluator(joke).result()\\n        if feedback.grade == \"funny\":\\n            break\\n\\n    return joke\\n\\n# Invoke\\nfor step in optimizer_workflow.stream(\"Cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/f66830be-4339-4a6b-8a93-389ce5ae27b4/r\\n\\n\\n\\nAgent¶\\nAgents are typically implemented as an LLM performing actions (via tool-calling) based on environmental feedback in a loop. As noted in the Anthropic blog on Building Effective Agents:\\n\\nAgents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully.\\nWhen to use agents: Agents can be used for open-ended problems where it\\'s difficult or impossible to predict the required number of steps, and where you can\\'t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents\\' autonomy makes them ideal for scaling tasks in trusted environments.\\n\\n\\nAPI Reference: tool\\nfrom langchain_core.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional API\\n\\n\\nfrom langgraph.graph import MessagesState\\nfrom langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\n\\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"Action\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"Action\"\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"environment\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    {\\n        # Name returned by should_continue : Name of next node to visit\\n        \"Action\": \"environment\",\\n        END: END,\\n    },\\n)\\nagent_builder.add_edge(\"environment\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/051f0391-6761-4f8c-a53b-22231b016690/r\\nResources:\\nLangChain Academy\\nSee our lesson on agents here.\\nExamples\\nHere is a project that uses a tool calling agent to create / store long-term memories.\\n\\n\\nfrom langgraph.graph import add_messages\\nfrom langchain_core.messages import (\\n    SystemMessage,\\n    HumanMessage,\\n    BaseMessage,\\n    ToolCall,\\n)\\n\\n\\n@task\\ndef call_llm(messages: list[BaseMessage]):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n    return llm_with_tools.invoke(\\n        [\\n            SystemMessage(\\n                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n            )\\n        ]\\n        + messages\\n    )\\n\\n\\n@task\\ndef call_tool(tool_call: ToolCall):\\n    \"\"\"Performs the tool call\"\"\"\\n    tool = tools_by_name[tool_call[\"name\"]]\\n    return tool.invoke(tool_call)\\n\\n\\n@entrypoint()\\ndef agent(messages: list[BaseMessage]):\\n    llm_response = call_llm(messages).result()\\n\\n    while True:\\n        if not llm_response.tool_calls:\\n            break\\n\\n        # Execute tools\\n        tool_result_futures = [\\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\\n        ]\\n        tool_results = [fut.result() for fut in tool_result_futures]\\n        messages = add_messages(messages, [llm_response, *tool_results])\\n        llm_response = call_llm(messages).result()\\n\\n    messages = add_messages(messages, llm_response)\\n    return messages\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nfor chunk in agent.stream(messages, stream_mode=\"updates\"):\\n    print(chunk)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/42ae8bf9-3935-4504-a081-8ddbcbfc8b2e/r\\n\\n\\n\\nPre-built¶\\nLangGraph also provides a pre-built method for creating an agent as defined above (using the create_react_agent function):\\nhttps://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\\nAPI Reference: create_react_agent\\nfrom langgraph.prebuilt import create_react_agent\\n\\n# Pass in:\\n# (1) the augmented LLM with tools\\n# (2) the tools list (which is used to create the tool node)\\npre_built_agent = create_react_agent(llm, tools=tools)\\n\\n# Show the agent\\ndisplay(Image(pre_built_agent.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = pre_built_agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/abab6a44-29f6-4b97-8164-af77413e494d/r\\nWhat LangGraph provides¶\\nBy constructing each of the above in LangGraph, we get a few things:\\nPersistence: Human-in-the-Loop¶\\nLangGraph persistence layer supports interruption and approval of actions (e.g., Human In The Loop). See Module 3 of LangChain Academy.\\nPersistence: Memory¶\\nLangGraph persistence layer supports conversational (short-term) memory and long-term memory. See Modules 2 and 5 of LangChain Academy:\\nStreaming¶\\nLangGraph provides several ways to stream workflow / agent outputs or intermediate state. See Module 3 of LangChain Academy.\\nDeployment¶\\nLangGraph provides an easy on-ramp for deployment, observability, and evaluation. See module 6 of LangChain Academy.\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Run a local server\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Agent architectures\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'), Document(metadata={'source': 'https://langchain-ai.github.io/langgraphjs/how-tos/map-reduce/', 'title': 'How to create map-reduce branches for parallel execution', 'description': 'Build language agents as graphs', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow to create map-reduce branches for parallel execution\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nThese docs will be deprecated and removed with the release of LangGraph v1.0 in October 2025. Visit the v1.0 alpha docs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n              How to create map-reduce branches for parallel execution\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n    \\n  \\n  LangGraph\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Agents\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  API reference\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Versions\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n  \\n    LangGraph\\n  \\n\\n          \\n\\n\\n\\n\\n\\n    \\n  \\n    Get started\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Get started\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    Learn the basics\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Deployment\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Guides\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Guides\\n  \\n\\n          \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    How-to Guides\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n  \\n    How-to Guides\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    Installation\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    LangGraph\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    LangGraph\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Controllability\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Controllability\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    Controllability\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    How to create map-reduce branches for parallel execution\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n    \\n  \\n    How to create map-reduce branches for parallel execution\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      \\n        Setup\\n      \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    How to create branches for parallel node execution\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    How to combine control flow and state updates with Command\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    How to create and control loops\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    How to defer node execution\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Persistence\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Memory\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Human-in-the-loop\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Streaming\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Tool calling\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Subgraphs\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Multi-agent\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    State Management\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Other\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Prebuilt ReAct Agent\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph Platform\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Concepts\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Tutorials\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Resources\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Resources\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    Adopters\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    LLMS-txt\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    FAQ\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Troubleshooting\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph Academy Course\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Agents\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    API reference\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Versions\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      \\n        Setup\\n      \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n    Guides\\n  \\n\\n\\n\\n\\n\\n    How-to Guides\\n  \\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n    Controllability\\n  \\n\\n\\n\\n\\n\\n\\n\\nHow to create map-reduce branches for parallel execution¶\\nMap-reduce operations are essential for efficient task decomposition and parallel processing. This approach involves breaking a task into smaller sub-tasks, processing each sub-task in parallel, and aggregating the results across all of the completed sub-tasks. \\nConsider this example: given a general topic from the user, generate a list of related subjects, generate a joke for each subject, and select the best joke from the resulting list. In this design pattern, a first node may generate a list of objects (e.g., related subjects) and we want to apply some other node (e.g., generate a joke) to all those objects (e.g., subjects). However, two main challenges arise.\\n(1) the number of objects (e.g., subjects) may be unknown ahead of time (meaning the number of edges may not be known) when we lay out the graph and (2) the input State to the downstream Node should be different (one for each generated object).\\nLangGraph addresses these challenges through its Send API. By utilizing conditional edges, Send can distribute different states (e.g., subjects) to multiple instances of a node (e.g., joke generation). Importantly, the sent state can differ from the core graph\\'s state, allowing for flexible and dynamic workflow management. \\n\\nSetup¶\\nThis example will require a few dependencies. First, install the LangGraph library, along with the @langchain/anthropic package as we\\'ll be using Anthropic LLMs in this example:\\nnpm install @langchain/langgraph @langchain/anthropic @langchain/core\\n\\nNext, set your Anthropic API key:\\nprocess.env.ANTHROPIC_API_KEY = \\'YOUR_API_KEY\\'\\n\\nimport { z } from \"zod\";\\nimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport { StateGraph, END, START, Annotation, Send } from \"@langchain/langgraph\";\\n\\n/* Model and prompts */\\n\\n// Define model and prompts we will use\\nconst subjectsPrompt = \"Generate a comma separated list of between 2 and 5 examples related to: {topic}.\"\\nconst jokePrompt = \"Generate a joke about {subject}\"\\nconst bestJokePrompt = `Below are a bunch of jokes about {topic}. Select the best one! Return the ID (index) of the best one.\\n\\n{jokes}`\\n\\n// Zod schemas for getting structured output from the LLM\\nconst Subjects = z.object({\\n  subjects: z.array(z.string()),\\n});\\nconst Joke = z.object({\\n  joke: z.string(),\\n});\\nconst BestJoke = z.object({\\n  id: z.number(),\\n});\\n\\nconst model = new ChatAnthropic({\\n  model: \"claude-3-5-sonnet-20240620\",\\n});\\n\\n/* Graph components: define the components that will make up the graph */\\n\\n// This will be the overall state of the main graph.\\n// It will contain a topic (which we expect the user to provide)\\n// and then will generate a list of subjects, and then a joke for\\n// each subject\\nconst OverallState = Annotation.Root({\\n  topic: Annotation<string>,\\n  subjects: Annotation<string[]>,\\n  // Notice here we pass a reducer function.\\n  // This is because we want combine all the jokes we generate\\n  // from individual nodes back into one list.\\n  jokes: Annotation<string[]>({\\n    reducer: (state, update) => state.concat(update),\\n  }),\\n  bestSelectedJoke: Annotation<string>,\\n});\\n\\n// This will be the state of the node that we will \"map\" all\\n// subjects to in order to generate a joke\\ninterface JokeState {\\n  subject: string;\\n}\\n\\n// This is the function we will use to generate the subjects of the jokes\\nconst generateTopics = async (\\n  state: typeof OverallState.State\\n): Promise<Partial<typeof OverallState.State>> => {\\n  const prompt = subjectsPrompt.replace(\"topic\", state.topic);\\n  const response = await model\\n    .withStructuredOutput(Subjects, { name: \"subjects\" })\\n    .invoke(prompt);\\n  return { subjects: response.subjects };\\n};\\n\\n// Function to generate a joke\\nconst generateJoke = async (state: JokeState): Promise<{ jokes: string[] }> => {\\n  const prompt = jokePrompt.replace(\"subject\", state.subject);\\n  const response = await model\\n    .withStructuredOutput(Joke, { name: \"joke\" })\\n    .invoke(prompt);\\n  return { jokes: [response.joke] };\\n};\\n\\n// Here we define the logic to map out over the generated subjects\\n// We will use this an edge in the graph\\nconst continueToJokes = (state: typeof OverallState.State) => {\\n  // We will return a list of `Send` objects\\n  // Each `Send` object consists of the name of a node in the graph\\n  // as well as the state to send to that node\\n  return state.subjects.map((subject) => new Send(\"generateJoke\", { subject }));\\n};\\n\\n// Here we will judge the best joke\\nconst bestJoke = async (\\n  state: typeof OverallState.State\\n): Promise<Partial<typeof OverallState.State>> => {\\n  const jokes = state.jokes.join(\"\\\\n\\\\n\");\\n  const prompt = bestJokePrompt\\n    .replace(\"jokes\", jokes)\\n    .replace(\"topic\", state.topic);\\n  const response = await model\\n    .withStructuredOutput(BestJoke, { name: \"best_joke\" })\\n    .invoke(prompt);\\n  return { bestSelectedJoke: state.jokes[response.id] };\\n};\\n\\n// Construct the graph: here we put everything together to construct our graph\\nconst graph = new StateGraph(OverallState)\\n  .addNode(\"generateTopics\", generateTopics)\\n  .addNode(\"generateJoke\", generateJoke)\\n  .addNode(\"bestJoke\", bestJoke)\\n  .addEdge(START, \"generateTopics\")\\n  .addConditionalEdges(\"generateTopics\", continueToJokes)\\n  .addEdge(\"generateJoke\", \"bestJoke\")\\n  .addEdge(\"bestJoke\", END);\\n\\nconst app = graph.compile();\\n\\nimport * as tslab from \"tslab\";\\n\\nconst representation = app.getGraph();\\nconst image = await representation.drawMermaidPng();\\nconst arrayBuffer = await image.arrayBuffer();\\n\\ntslab.display.png(new Uint8Array(arrayBuffer));\\n\\n\\n// Call the graph: here we call it to generate a list of jokes\\nfor await (const s of await app.stream({ topic: \"animals\" })) {\\n  console.log(s);\\n}\\n\\n{\\n  generateTopics: { subjects: [ \\'lion\\', \\'elephant\\', \\'penguin\\', \\'dolphin\\' ] }\\n}\\n{\\n  generateJoke: {\\n    jokes: [ \"Why don\\'t lions like fast food? Because they can\\'t catch it!\" ]\\n  }\\n}\\n{\\n  generateJoke: {\\n    jokes: [\\n      \"Why don\\'t elephants use computers? Because they\\'re afraid of the mouse!\"\\n    ]\\n  }\\n}\\n{\\n  generateJoke: {\\n    jokes: [\\n      \"Why don\\'t dolphins use smartphones? They\\'re afraid of phishing!\"\\n    ]\\n  }\\n}\\n{\\n  generateJoke: {\\n    jokes: [\\n      \"Why don\\'t you see penguins in Britain? Because they\\'re afraid of Wales!\"\\n    ]\\n  }\\n}\\n{\\n  bestJoke: {\\n    bestSelectedJoke: \"Why don\\'t elephants use computers? Because they\\'re afraid of the mouse!\"\\n  }\\n}\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                How to use LangGraph.js in web environments\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                How to create branches for parallel node execution\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs Insiders\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCookie consent\\nWe use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they\\'re searching for. Clicking \"Accept\" makes our documentation better. Thank you! ❤️\\n\\n\\n\\n\\n\\n\\n\\n          GitHub\\n        \\n\\n\\n\\n\\nAccept\\nReject\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]\n",
            "[Document(metadata={'source': 'https://academy.langchain.com/courses/intro-to-langgraph', 'title': 'Foundation: Introduction to LangGraph', 'description': 'Learn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.', 'language': 'en'}, page_content='Foundation: Introduction to LangGraph\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\nPython\\n\\nLangChain\\nLangSmith\\nLangGraph\\n\\n\\n\\nJavaScript\\n\\nLangChain\\nLangSmith\\nLangGraph\\n\\n\\n\\n\\nCommunity\\nLangSmith\\nAll Courses\\n\\n\\nSign In\\n\\n\\nRegister\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Academy\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFoundation: Introduction to LangGraph\\nLearn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.\\n\\n\\n\\nEnroll for free\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWatch Intro Video\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCourse Curriculum\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Welcome to the course!\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nCourse Overview\\n\\n\\n\\n\\n\\n\\n\\nGetting Set Up\\n\\n\\n\\n\\n\\n\\n\\nGetting Set Up (Video Guide)\\n\\n\\n\\n\\n\\n\\n\\nModule 0 Resources\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 1: Introduction\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 1 Introduction'), Document(metadata={'source': 'https://academy.langchain.com/courses/intro-to-langgraph', 'title': 'Foundation: Introduction to LangGraph', 'description': 'Learn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.', 'language': 'en'}, page_content='Module 1 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 1 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Motivation\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: Simple Graph\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: LangSmith Studio\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Chain\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Router\\n\\n\\n\\n\\n\\n\\n\\nLesson 6: Agent\\n\\n\\n\\n\\n\\n\\n\\nLesson 7: Agent with Memory\\n\\n\\n\\n\\n\\n\\n\\n[Optional] Lesson 8: Intro to Deployment\\n\\n\\n\\n\\n\\n\\n\\nModule 1 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 2: State and Memory\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 2 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 2 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: State Schema\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: State Reducers\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Multiple Schemas\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Trim and Filter Messages\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Chatbot w/ Summarizing Messages and Memory\\n\\n\\n\\n\\n\\n\\n\\nLesson 6: Chatbot w/ Summarizing Messages and External Memory\\n\\n\\n\\n\\n\\n\\n\\nModule 2 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 3: UX and Human-in-the-Loop\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 3 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 3 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Streaming\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: Breakpoints')]\n"
          ]
        }
      ],
      "source": [
        "doc_list=[item for sublist in docs for item in sublist]\n",
        "print(doc_list)\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
        "\n",
        "doc_splits=text_splitter.split_documents(doc_list)\n",
        "print(doc_splits[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "62db2b5d",
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorstore=FAISS.from_documents(\n",
        "    documents=doc_splits,\n",
        "    embedding=OpenAIEmbeddings()\n",
        ")\n",
        "retriever=vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3955286b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='632ae130-e85c-4928-a695-22eb8c25afd2', metadata={'source': 'https://academy.langchain.com/courses/intro-to-langgraph', 'title': 'Foundation: Introduction to LangGraph', 'description': 'Learn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.', 'language': 'en'}, page_content='No. LangGraph is an orchestration framework for complex agentic systems and is more low-level and controllable than LangChain agents. On the other hand, LangChain provides a standard interface to interact with models and other components, useful for straight-forward chains and retrieval flows.\\n\\n\\n\\n\\n\\n\\n\\n                      How is LangGraph different from other agent frameworks?\\n                    \\n\\nOther agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a company’s needs. LangGraph provides a more expressive framework to handle companies’ unique tasks without restricting users to a single black-box cognitive architecture.\\n\\n\\n\\n\\n\\n\\n\\n                      Does LangGraph impact the performance of my app?\\n                    \\n\\nLangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.\\n\\n\\n\\n\\n\\n\\n\\n                      Is LangGraph open source? Is it free?'),\n",
              " Document(id='0bfafafc-7694-4e27-a1df-206a5158488f', metadata={'source': 'https://academy.langchain.com/courses/intro-to-langgraph', 'title': 'Foundation: Introduction to LangGraph', 'description': 'Learn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.', 'language': 'en'}, page_content='Foundation: Introduction to LangGraph\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\nPython\\n\\nLangChain\\nLangSmith\\nLangGraph\\n\\n\\n\\nJavaScript\\n\\nLangChain\\nLangSmith\\nLangGraph\\n\\n\\n\\n\\nCommunity\\nLangSmith\\nAll Courses\\n\\n\\nSign In\\n\\n\\nRegister\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Academy\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFoundation: Introduction to LangGraph\\nLearn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.\\n\\n\\n\\nEnroll for free\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWatch Intro Video\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCourse Curriculum\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Welcome to the course!\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nCourse Overview\\n\\n\\n\\n\\n\\n\\n\\nGetting Set Up\\n\\n\\n\\n\\n\\n\\n\\nGetting Set Up (Video Guide)\\n\\n\\n\\n\\n\\n\\n\\nModule 0 Resources\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 1: Introduction\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 1 Introduction'),\n",
              " Document(id='e713fa4e-96dd-4fcd-a9ab-639baa4e80ea', metadata={'source': 'https://academy.langchain.com/courses/intro-to-langgraph', 'title': 'Foundation: Introduction to LangGraph', 'description': 'Learn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.', 'language': 'en'}, page_content='Is LangGraph open source? Is it free?\\n                    \\n\\nYes. LangGraph is an MIT-licensed open-source library and is free to use.\\n\\n\\n\\n\\n\\n\\n\\n                      What is LangSmith Deployment?\\n                    \\n\\nLangSmith Deployment helps you ship your agent in one click, using scalable infrastructure built for long-running tasks.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReady to start shipping reliable agents faster?\\nOur platform provides tools for every step of the agent development lifecycle — built to unlock powerful AI in production.\\n\\n\\n\\n                Contact Sales\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLearn with the community\\nLearn alongside other builders at in-person LangChain meetups. Subscribe to our calendar to be the first to know about upcoming events near you.\\n\\n\\n\\n                Subscribe\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n            © Copyright LangChain Academy 2025'),\n",
              " Document(id='8428b1f9-4d70-4834-846c-f8987bd83beb', metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Workflows & agents', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='LangSmith Trace\\nhttps://smith.langchain.com/public/abab6a44-29f6-4b97-8164-af77413e494d/r\\nWhat LangGraph provides¶\\nBy constructing each of the above in LangGraph, we get a few things:\\nPersistence: Human-in-the-Loop¶\\nLangGraph persistence layer supports interruption and approval of actions (e.g., Human In The Loop). See Module 3 of LangChain Academy.\\nPersistence: Memory¶\\nLangGraph persistence layer supports conversational (short-term) memory and long-term memory. See Modules 2 and 5 of LangChain Academy:\\nStreaming¶\\nLangGraph provides several ways to stream workflow / agent outputs or intermediate state. See Module 3 of LangChain Academy.\\nDeployment¶\\nLangGraph provides an easy on-ramp for deployment, observability, and evaluation. See module 6 of LangChain Academy.\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Run a local server\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Agent architectures')]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.invoke(\"what is langgraph\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3d18b8d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Retriever to Retriever tools\n",
        "\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "retriever_tool=create_retriever_tool(\n",
        "    retriever,\n",
        "    \"retriver_vector_db_blog\",\n",
        "    \"search and run information about langgraph\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "68925bfe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(metadata={'source': 'https://academy.langchain.com/courses/intro-to-langgraph', 'title': 'Foundation: Introduction to LangGraph', 'description': 'Learn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFoundation: Introduction to LangGraph\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\nPython\\n\\nLangChain\\nLangSmith\\nLangGraph\\n\\n\\n\\nJavaScript\\n\\nLangChain\\nLangSmith\\nLangGraph\\n\\n\\n\\n\\nCommunity\\nLangSmith\\nAll Courses\\n\\n\\nSign In\\n\\n\\nRegister\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Academy\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFoundation: Introduction to LangGraph\\nLearn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.\\n\\n\\n\\nEnroll for free\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWatch Intro Video\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCourse Curriculum\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Welcome to the course!\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nCourse Overview\\n\\n\\n\\n\\n\\n\\n\\nGetting Set Up\\n\\n\\n\\n\\n\\n\\n\\nGetting Set Up (Video Guide)\\n\\n\\n\\n\\n\\n\\n\\nModule 0 Resources\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 1: Introduction\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 1 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 1 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Motivation\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: Simple Graph\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: LangSmith Studio\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Chain\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Router\\n\\n\\n\\n\\n\\n\\n\\nLesson 6: Agent\\n\\n\\n\\n\\n\\n\\n\\nLesson 7: Agent with Memory\\n\\n\\n\\n\\n\\n\\n\\n[Optional] Lesson 8: Intro to Deployment\\n\\n\\n\\n\\n\\n\\n\\nModule 1 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 2: State and Memory\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 2 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 2 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: State Schema\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: State Reducers\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Multiple Schemas\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Trim and Filter Messages\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Chatbot w/ Summarizing Messages and Memory\\n\\n\\n\\n\\n\\n\\n\\nLesson 6: Chatbot w/ Summarizing Messages and External Memory\\n\\n\\n\\n\\n\\n\\n\\nModule 2 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 3: UX and Human-in-the-Loop\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 3 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 3 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Streaming\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: Breakpoints\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Editing State and Human Feedback\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Dynamic Breakpoints\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Time Travel\\n\\n\\n\\n\\n\\n\\n\\nModule 3 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 4: Building Your Assistant\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 4 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 4 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Parallelization\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: Sub-graphs\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Map-reduce\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Research Assistant\\n\\n\\n\\n\\n\\n\\n\\nModule 4 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 5: Long-Term Memory\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 5 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Short vs. Long-Term Memory\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: LangGraph Store\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Memory Schema + Profile\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Memory Schema + Collection\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Build an Agent with Long-Term Memory\\n\\n\\n\\n\\n\\n\\n\\nModule 5 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 6: Deployment\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 6 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Deployment Concepts\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: Creating a Deployment\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Connecting to a Deployment\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Double Texting\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Assistants\\n\\n\\n\\n\\n\\n\\n\\nModule 6 Feedback\\n\\n\\n\\n\\n\\n\\n\\nEnd of Course Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Show more\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        About this course\\n      \\n\\n\\n\\n\\nFree\\n\\n\\n\\n\\n            54 lessons\\n          \\n\\n\\n\\n\\n            \\n            6 hours of video content\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph FAQs\\n\\n\\n\\n\\n\\n\\n\\n\\n                      Do I need to use LangChain to use LangGraph? What’s the difference?\\n                    \\n\\nNo. LangGraph is an orchestration framework for complex agentic systems and is more low-level and controllable than LangChain agents. On the other hand, LangChain provides a standard interface to interact with models and other components, useful for straight-forward chains and retrieval flows.\\n\\n\\n\\n\\n\\n\\n\\n                      How is LangGraph different from other agent frameworks?\\n                    \\n\\nOther agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a company’s needs. LangGraph provides a more expressive framework to handle companies’ unique tasks without restricting users to a single black-box cognitive architecture.\\n\\n\\n\\n\\n\\n\\n\\n                      Does LangGraph impact the performance of my app?\\n                    \\n\\nLangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.\\n\\n\\n\\n\\n\\n\\n\\n                      Is LangGraph open source? Is it free?\\n                    \\n\\nYes. LangGraph is an MIT-licensed open-source library and is free to use.\\n\\n\\n\\n\\n\\n\\n\\n                      What is LangSmith Deployment?\\n                    \\n\\nLangSmith Deployment helps you ship your agent in one click, using scalable infrastructure built for long-running tasks.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReady to start shipping reliable agents faster?\\nOur platform provides tools for every step of the agent development lifecycle — built to unlock powerful AI in production.\\n\\n\\n\\n                Contact Sales\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLearn with the community\\nLearn alongside other builders at in-person LangChain meetups. Subscribe to our calendar to be the first to know about upcoming events near you.\\n\\n\\n\\n                Subscribe\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n            © Copyright LangChain Academy 2025\\n          \\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'), Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Workflows & agents', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows & agents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nThese docs will be deprecated and removed with the release of LangGraph v1.0 in October 2025. Visit the v1.0 alpha docs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Workflows & agents\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n    Build a custom workflow\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Run a local server\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    General concepts\\n    \\n  \\n\\n\\n\\n\\n\\n            General concepts\\n          \\n\\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows and Agents¶\\nThis guide reviews common patterns for agentic systems. In describing these systems, it can be useful to make a distinction between \"workflows\" and \"agents\". One way to think about this difference is nicely explained in Anthropic\\'s Building Effective Agents blog post:\\n\\nWorkflows are systems where LLMs and tools are orchestrated through predefined code paths.\\nAgents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\\n\\nHere is a simple way to visualize these differences:\\n\\nWhen building agents and workflows, LangGraph offers a number of benefits including persistence, streaming, and support for debugging as well as deployment.\\nSet up¶\\nYou can use any chat model that supports structured outputs and tool calling. Below, we show the process of installing the packages, setting API keys, and testing structured outputs / tool calling for Anthropic.\\n\\nInstall dependencies\\npip install langchain_core langchain-anthropic langgraph\\n\\n\\nInitialize an LLM\\nAPI Reference: ChatAnthropic\\nimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\\n\\nBuilding Blocks: The Augmented LLM¶\\nLLM have augmentations that support building workflows and agents. These include structured outputs and tool calling, as shown in this image from the Anthropic blog on Building Effective Agents:\\n\\n# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\nPrompt chaining¶\\nIn prompt chaining, each LLM call processes the output of the previous one.\\nAs noted in the Anthropic blog on Building Effective Agents:\\n\\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate\" in the diagram below) on any intermediate steps to ensure that the process is still on track.\\nWhen to use this workflow: This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Joke failed quality gate - no punchline detected!\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/a0281fca-3a71-46de-beee-791468607b75/r\\nResources:\\nLangChain Academy\\nSee our lesson on Prompt Chaining here.\\n\\n\\nfrom langgraph.func import entrypoint, task\\n\\n\\n# Tasks\\n@task\\ndef generate_joke(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a short joke about {topic}\")\\n    return msg.content\\n\\n\\ndef check_punchline(joke: str):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in joke or \"!\" in joke:\\n        return \"Fail\"\\n\\n    return \"Pass\"\\n\\n\\n@task\\ndef improve_joke(joke: str):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\\n    return msg.content\\n\\n\\n@task\\ndef polish_joke(joke: str):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\\n    return msg.content\\n\\n\\n@entrypoint()\\ndef prompt_chaining_workflow(topic: str):\\n    original_joke = generate_joke(topic).result()\\n    if check_punchline(original_joke) == \"Pass\":\\n        return original_joke\\n\\n    improved_joke = improve_joke(original_joke).result()\\n    return polish_joke(improved_joke).result()\\n\\n# Invoke\\nfor step in prompt_chaining_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/332fa4fc-b6ca-416e-baa3-161625e69163/r\\n\\n\\n\\nParallelization¶\\nWith parallelization, LLMs work simultaneously on a task:\\n\\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations: Sectioning: Breaking a task into independent subtasks run in parallel. Voting: Running the same task multiple times to get diverse outputs.\\nWhen to use this workflow: Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/3be2e53c-ca94-40dd-934f-82ff87fac277/r\\nResources:\\nDocumentation\\nSee our documentation on parallelization here.\\nLangChain Academy\\nSee our lesson on parallelization here.\\n\\n\\n@task\\ndef call_llm_1(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_2(topic: str):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n    msg = llm.invoke(f\"Write a story about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_3(topic):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n    msg = llm.invoke(f\"Write a poem about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef aggregator(topic, joke, story, poem):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {topic}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{story}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{joke}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{poem}\"\\n    return combined\\n\\n\\n# Build workflow\\n@entrypoint()\\ndef parallel_workflow(topic: str):\\n    joke_fut = call_llm_1(topic)\\n    story_fut = call_llm_2(topic)\\n    poem_fut = call_llm_3(topic)\\n    return aggregator(\\n        topic, joke_fut.result(), story_fut.result(), poem_fut.result()\\n    ).result()\\n\\n# Invoke\\nfor step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/623d033f-e814-41e9-80b1-75e6abb67801/r\\n\\n\\n\\nRouting¶\\nRouting classifies an input and directs it to a followup task. As noted in the Anthropic blog on Building Effective Agents:\\n\\nRouting classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.\\nWhen to use this workflow: Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import Literal\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\\n\\n# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/c4580b74-fe91-47e4-96fe-7fac598d509c/r\\nResources:\\nLangChain Academy\\nSee our lesson on routing here.\\nExamples\\nHere is RAG workflow that routes questions. See our video here.\\n\\n\\nfrom typing_extensions import Literal\\nfrom pydantic import BaseModel\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n@task\\ndef llm_call_1(input_: str):\\n    \"\"\"Write a story\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_2(input_: str):\\n    \"\"\"Write a joke\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_3(input_: str):\\n    \"\"\"Write a poem\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\ndef llm_call_router(input_: str):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=input_),\\n        ]\\n    )\\n    return decision.step\\n\\n\\n# Create workflow\\n@entrypoint()\\ndef router_workflow(input_: str):\\n    next_step = llm_call_router(input_)\\n    if next_step == \"story\":\\n        llm_call = llm_call_1\\n    elif next_step == \"joke\":\\n        llm_call = llm_call_2\\n    elif next_step == \"poem\":\\n        llm_call = llm_call_3\\n\\n    return llm_call(input_).result()\\n\\n# Invoke\\nfor step in router_workflow.stream(\"Write me a joke about cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/5e2eb979-82dd-402c-b1a0-a8cceaf2a28a/r\\n\\n\\n\\nOrchestrator-Worker¶\\nWith orchestrator-worker, an orchestrator breaks down a task and delegates each sub-task to workers. As noted in the Anthropic blog on Building Effective Agents:\\n\\nIn the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\\nWhen to use this workflow: This workflow is well-suited for complex tasks where you can\\'t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it\\'s topographically similar, the key difference from parallelization is its flexibility—subtasks aren\\'t pre-defined, but determined by the orchestrator based on the specific input.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\nCreating Workers in LangGraph\\nBecause orchestrator-worker workflows are common, LangGraph has the Send API to support this. It lets you dynamically create worker nodes and send each one a specific input. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. As you can see below, we iterate over a list of sections and Send each to a worker node. See further documentation here and here.\\nfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report\\n\\n\\n# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n\\n    return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\\n\\n# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/78cbcfc3-38bf-471d-b62a-b299b144237d/r\\nResources:\\nLangChain Academy\\nSee our lesson on orchestrator-worker here.\\nExamples\\nHere is a project that uses orchestrator-worker for report planning and writing. See our video here.\\n\\n\\nfrom typing import List\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\n@task\\ndef orchestrator(topic: str):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {topic}\"),\\n        ]\\n    )\\n\\n    return report_sections.sections\\n\\n\\n@task\\ndef llm_call(section: Section):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    result = llm.invoke(\\n        [\\n            SystemMessage(content=\"Write a report section.\"),\\n            HumanMessage(\\n                content=f\"Here is the section name: {section.name} and description: {section.description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return result.content\\n\\n\\n@task\\ndef synthesizer(completed_sections: list[str]):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n    final_report = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n    return final_report\\n\\n\\n@entrypoint()\\ndef orchestrator_worker(topic: str):\\n    sections = orchestrator(topic).result()\\n    section_futures = [llm_call(section) for section in sections]\\n    final_report = synthesizer(\\n        [section_fut.result() for section_fut in section_futures]\\n    ).result()\\n    return final_report\\n\\n# Invoke\\nreport = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\\nfrom IPython.display import Markdown\\nMarkdown(report)\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/75a636d0-6179-4a12-9836-e0aa571e87c5/r\\n\\n\\n\\nEvaluator-optimizer¶\\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop:\\n\\nWhen to use this workflow: This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\\n\\n\\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)\\n\\n# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/86ab3e60-2000-4bff-b988-9b89a3269789/r\\nResources:\\nExamples\\nHere is an assistant that uses evaluator-optimizer to improve a report. See our video here.\\nHere is a RAG workflow that grades answers for hallucinations or errors. See our video here.\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\n@task\\ndef llm_call_generator(topic: str, feedback: Feedback):\\n    \"\"\"LLM generates a joke\"\"\"\\n    if feedback:\\n        msg = llm.invoke(\\n            f\"Write a joke about {topic} but take into account the feedback: {feedback}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef llm_call_evaluator(joke: str):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n    feedback = evaluator.invoke(f\"Grade the joke {joke}\")\\n    return feedback\\n\\n\\n@entrypoint()\\ndef optimizer_workflow(topic: str):\\n    feedback = None\\n    while True:\\n        joke = llm_call_generator(topic, feedback).result()\\n        feedback = llm_call_evaluator(joke).result()\\n        if feedback.grade == \"funny\":\\n            break\\n\\n    return joke\\n\\n# Invoke\\nfor step in optimizer_workflow.stream(\"Cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/f66830be-4339-4a6b-8a93-389ce5ae27b4/r\\n\\n\\n\\nAgent¶\\nAgents are typically implemented as an LLM performing actions (via tool-calling) based on environmental feedback in a loop. As noted in the Anthropic blog on Building Effective Agents:\\n\\nAgents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully.\\nWhen to use agents: Agents can be used for open-ended problems where it\\'s difficult or impossible to predict the required number of steps, and where you can\\'t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents\\' autonomy makes them ideal for scaling tasks in trusted environments.\\n\\n\\nAPI Reference: tool\\nfrom langchain_core.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional API\\n\\n\\nfrom langgraph.graph import MessagesState\\nfrom langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\n\\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"Action\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"Action\"\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"environment\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    {\\n        # Name returned by should_continue : Name of next node to visit\\n        \"Action\": \"environment\",\\n        END: END,\\n    },\\n)\\nagent_builder.add_edge(\"environment\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/051f0391-6761-4f8c-a53b-22231b016690/r\\nResources:\\nLangChain Academy\\nSee our lesson on agents here.\\nExamples\\nHere is a project that uses a tool calling agent to create / store long-term memories.\\n\\n\\nfrom langgraph.graph import add_messages\\nfrom langchain_core.messages import (\\n    SystemMessage,\\n    HumanMessage,\\n    BaseMessage,\\n    ToolCall,\\n)\\n\\n\\n@task\\ndef call_llm(messages: list[BaseMessage]):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n    return llm_with_tools.invoke(\\n        [\\n            SystemMessage(\\n                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n            )\\n        ]\\n        + messages\\n    )\\n\\n\\n@task\\ndef call_tool(tool_call: ToolCall):\\n    \"\"\"Performs the tool call\"\"\"\\n    tool = tools_by_name[tool_call[\"name\"]]\\n    return tool.invoke(tool_call)\\n\\n\\n@entrypoint()\\ndef agent(messages: list[BaseMessage]):\\n    llm_response = call_llm(messages).result()\\n\\n    while True:\\n        if not llm_response.tool_calls:\\n            break\\n\\n        # Execute tools\\n        tool_result_futures = [\\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\\n        ]\\n        tool_results = [fut.result() for fut in tool_result_futures]\\n        messages = add_messages(messages, [llm_response, *tool_results])\\n        llm_response = call_llm(messages).result()\\n\\n    messages = add_messages(messages, llm_response)\\n    return messages\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nfor chunk in agent.stream(messages, stream_mode=\"updates\"):\\n    print(chunk)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/42ae8bf9-3935-4504-a081-8ddbcbfc8b2e/r\\n\\n\\n\\nPre-built¶\\nLangGraph also provides a pre-built method for creating an agent as defined above (using the create_react_agent function):\\nhttps://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\\nAPI Reference: create_react_agent\\nfrom langgraph.prebuilt import create_react_agent\\n\\n# Pass in:\\n# (1) the augmented LLM with tools\\n# (2) the tools list (which is used to create the tool node)\\npre_built_agent = create_react_agent(llm, tools=tools)\\n\\n# Show the agent\\ndisplay(Image(pre_built_agent.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = pre_built_agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/abab6a44-29f6-4b97-8164-af77413e494d/r\\nWhat LangGraph provides¶\\nBy constructing each of the above in LangGraph, we get a few things:\\nPersistence: Human-in-the-Loop¶\\nLangGraph persistence layer supports interruption and approval of actions (e.g., Human In The Loop). See Module 3 of LangChain Academy.\\nPersistence: Memory¶\\nLangGraph persistence layer supports conversational (short-term) memory and long-term memory. See Modules 2 and 5 of LangChain Academy:\\nStreaming¶\\nLangGraph provides several ways to stream workflow / agent outputs or intermediate state. See Module 3 of LangChain Academy.\\nDeployment¶\\nLangGraph provides an easy on-ramp for deployment, observability, and evaluation. See module 6 of LangChain Academy.\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Run a local server\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Agent architectures\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'), Document(metadata={'source': 'https://langchain-ai.github.io/langgraphjs/how-tos/map-reduce/', 'title': 'How to create map-reduce branches for parallel execution', 'description': 'Build language agents as graphs', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow to create map-reduce branches for parallel execution\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nThese docs will be deprecated and removed with the release of LangGraph v1.0 in October 2025. Visit the v1.0 alpha docs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n              How to create map-reduce branches for parallel execution\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n    \\n  \\n  LangGraph\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Agents\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  API reference\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Versions\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n  \\n    LangGraph\\n  \\n\\n          \\n\\n\\n\\n\\n\\n    \\n  \\n    Get started\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Get started\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    Learn the basics\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Deployment\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Guides\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Guides\\n  \\n\\n          \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    How-to Guides\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n  \\n    How-to Guides\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    Installation\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    LangGraph\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    LangGraph\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Controllability\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Controllability\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    Controllability\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    How to create map-reduce branches for parallel execution\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n    \\n  \\n    How to create map-reduce branches for parallel execution\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      \\n        Setup\\n      \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    How to create branches for parallel node execution\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    How to combine control flow and state updates with Command\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    How to create and control loops\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    How to defer node execution\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Persistence\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Memory\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Human-in-the-loop\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Streaming\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Tool calling\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Subgraphs\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Multi-agent\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    State Management\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Other\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Prebuilt ReAct Agent\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph Platform\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Concepts\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Tutorials\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Resources\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Resources\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    Adopters\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    LLMS-txt\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    FAQ\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Troubleshooting\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph Academy Course\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Agents\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    API reference\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Versions\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      \\n        Setup\\n      \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n    Guides\\n  \\n\\n\\n\\n\\n\\n    How-to Guides\\n  \\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n    Controllability\\n  \\n\\n\\n\\n\\n\\n\\n\\nHow to create map-reduce branches for parallel execution¶\\nMap-reduce operations are essential for efficient task decomposition and parallel processing. This approach involves breaking a task into smaller sub-tasks, processing each sub-task in parallel, and aggregating the results across all of the completed sub-tasks. \\nConsider this example: given a general topic from the user, generate a list of related subjects, generate a joke for each subject, and select the best joke from the resulting list. In this design pattern, a first node may generate a list of objects (e.g., related subjects) and we want to apply some other node (e.g., generate a joke) to all those objects (e.g., subjects). However, two main challenges arise.\\n(1) the number of objects (e.g., subjects) may be unknown ahead of time (meaning the number of edges may not be known) when we lay out the graph and (2) the input State to the downstream Node should be different (one for each generated object).\\nLangGraph addresses these challenges through its Send API. By utilizing conditional edges, Send can distribute different states (e.g., subjects) to multiple instances of a node (e.g., joke generation). Importantly, the sent state can differ from the core graph\\'s state, allowing for flexible and dynamic workflow management. \\n\\nSetup¶\\nThis example will require a few dependencies. First, install the LangGraph library, along with the @langchain/anthropic package as we\\'ll be using Anthropic LLMs in this example:\\nnpm install @langchain/langgraph @langchain/anthropic @langchain/core\\n\\nNext, set your Anthropic API key:\\nprocess.env.ANTHROPIC_API_KEY = \\'YOUR_API_KEY\\'\\n\\nimport { z } from \"zod\";\\nimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport { StateGraph, END, START, Annotation, Send } from \"@langchain/langgraph\";\\n\\n/* Model and prompts */\\n\\n// Define model and prompts we will use\\nconst subjectsPrompt = \"Generate a comma separated list of between 2 and 5 examples related to: {topic}.\"\\nconst jokePrompt = \"Generate a joke about {subject}\"\\nconst bestJokePrompt = `Below are a bunch of jokes about {topic}. Select the best one! Return the ID (index) of the best one.\\n\\n{jokes}`\\n\\n// Zod schemas for getting structured output from the LLM\\nconst Subjects = z.object({\\n  subjects: z.array(z.string()),\\n});\\nconst Joke = z.object({\\n  joke: z.string(),\\n});\\nconst BestJoke = z.object({\\n  id: z.number(),\\n});\\n\\nconst model = new ChatAnthropic({\\n  model: \"claude-3-5-sonnet-20240620\",\\n});\\n\\n/* Graph components: define the components that will make up the graph */\\n\\n// This will be the overall state of the main graph.\\n// It will contain a topic (which we expect the user to provide)\\n// and then will generate a list of subjects, and then a joke for\\n// each subject\\nconst OverallState = Annotation.Root({\\n  topic: Annotation<string>,\\n  subjects: Annotation<string[]>,\\n  // Notice here we pass a reducer function.\\n  // This is because we want combine all the jokes we generate\\n  // from individual nodes back into one list.\\n  jokes: Annotation<string[]>({\\n    reducer: (state, update) => state.concat(update),\\n  }),\\n  bestSelectedJoke: Annotation<string>,\\n});\\n\\n// This will be the state of the node that we will \"map\" all\\n// subjects to in order to generate a joke\\ninterface JokeState {\\n  subject: string;\\n}\\n\\n// This is the function we will use to generate the subjects of the jokes\\nconst generateTopics = async (\\n  state: typeof OverallState.State\\n): Promise<Partial<typeof OverallState.State>> => {\\n  const prompt = subjectsPrompt.replace(\"topic\", state.topic);\\n  const response = await model\\n    .withStructuredOutput(Subjects, { name: \"subjects\" })\\n    .invoke(prompt);\\n  return { subjects: response.subjects };\\n};\\n\\n// Function to generate a joke\\nconst generateJoke = async (state: JokeState): Promise<{ jokes: string[] }> => {\\n  const prompt = jokePrompt.replace(\"subject\", state.subject);\\n  const response = await model\\n    .withStructuredOutput(Joke, { name: \"joke\" })\\n    .invoke(prompt);\\n  return { jokes: [response.joke] };\\n};\\n\\n// Here we define the logic to map out over the generated subjects\\n// We will use this an edge in the graph\\nconst continueToJokes = (state: typeof OverallState.State) => {\\n  // We will return a list of `Send` objects\\n  // Each `Send` object consists of the name of a node in the graph\\n  // as well as the state to send to that node\\n  return state.subjects.map((subject) => new Send(\"generateJoke\", { subject }));\\n};\\n\\n// Here we will judge the best joke\\nconst bestJoke = async (\\n  state: typeof OverallState.State\\n): Promise<Partial<typeof OverallState.State>> => {\\n  const jokes = state.jokes.join(\"\\\\n\\\\n\");\\n  const prompt = bestJokePrompt\\n    .replace(\"jokes\", jokes)\\n    .replace(\"topic\", state.topic);\\n  const response = await model\\n    .withStructuredOutput(BestJoke, { name: \"best_joke\" })\\n    .invoke(prompt);\\n  return { bestSelectedJoke: state.jokes[response.id] };\\n};\\n\\n// Construct the graph: here we put everything together to construct our graph\\nconst graph = new StateGraph(OverallState)\\n  .addNode(\"generateTopics\", generateTopics)\\n  .addNode(\"generateJoke\", generateJoke)\\n  .addNode(\"bestJoke\", bestJoke)\\n  .addEdge(START, \"generateTopics\")\\n  .addConditionalEdges(\"generateTopics\", continueToJokes)\\n  .addEdge(\"generateJoke\", \"bestJoke\")\\n  .addEdge(\"bestJoke\", END);\\n\\nconst app = graph.compile();\\n\\nimport * as tslab from \"tslab\";\\n\\nconst representation = app.getGraph();\\nconst image = await representation.drawMermaidPng();\\nconst arrayBuffer = await image.arrayBuffer();\\n\\ntslab.display.png(new Uint8Array(arrayBuffer));\\n\\n\\n// Call the graph: here we call it to generate a list of jokes\\nfor await (const s of await app.stream({ topic: \"animals\" })) {\\n  console.log(s);\\n}\\n\\n{\\n  generateTopics: { subjects: [ \\'lion\\', \\'elephant\\', \\'penguin\\', \\'dolphin\\' ] }\\n}\\n{\\n  generateJoke: {\\n    jokes: [ \"Why don\\'t lions like fast food? Because they can\\'t catch it!\" ]\\n  }\\n}\\n{\\n  generateJoke: {\\n    jokes: [\\n      \"Why don\\'t elephants use computers? Because they\\'re afraid of the mouse!\"\\n    ]\\n  }\\n}\\n{\\n  generateJoke: {\\n    jokes: [\\n      \"Why don\\'t dolphins use smartphones? They\\'re afraid of phishing!\"\\n    ]\\n  }\\n}\\n{\\n  generateJoke: {\\n    jokes: [\\n      \"Why don\\'t you see penguins in Britain? Because they\\'re afraid of Wales!\"\\n    ]\\n  }\\n}\\n{\\n  bestJoke: {\\n    bestSelectedJoke: \"Why don\\'t elephants use computers? Because they\\'re afraid of the mouse!\"\\n  }\\n}\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                How to use LangGraph.js in web environments\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                How to create branches for parallel node execution\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs Insiders\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCookie consent\\nWe use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they\\'re searching for. Clicking \"Accept\" makes our documentation better. Thank you! ❤️\\n\\n\\n\\n\\n\\n\\n\\n          GitHub\\n        \\n\\n\\n\\n\\nAccept\\nReject\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]\n",
            "[Document(metadata={'source': 'https://academy.langchain.com/courses/intro-to-langgraph', 'title': 'Foundation: Introduction to LangGraph', 'description': 'Learn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.', 'language': 'en'}, page_content='Foundation: Introduction to LangGraph\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\nPython\\n\\nLangChain\\nLangSmith\\nLangGraph\\n\\n\\n\\nJavaScript\\n\\nLangChain\\nLangSmith\\nLangGraph\\n\\n\\n\\n\\nCommunity\\nLangSmith\\nAll Courses\\n\\n\\nSign In\\n\\n\\nRegister\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Academy\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFoundation: Introduction to LangGraph\\nLearn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.\\n\\n\\n\\nEnroll for free\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWatch Intro Video\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCourse Curriculum\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Welcome to the course!\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nCourse Overview\\n\\n\\n\\n\\n\\n\\n\\nGetting Set Up\\n\\n\\n\\n\\n\\n\\n\\nGetting Set Up (Video Guide)\\n\\n\\n\\n\\n\\n\\n\\nModule 0 Resources\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 1: Introduction\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 1 Introduction'), Document(metadata={'source': 'https://academy.langchain.com/courses/intro-to-langgraph', 'title': 'Foundation: Introduction to LangGraph', 'description': 'Learn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.', 'language': 'en'}, page_content='Module 1 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 1 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Motivation\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: Simple Graph\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: LangSmith Studio\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Chain\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Router\\n\\n\\n\\n\\n\\n\\n\\nLesson 6: Agent\\n\\n\\n\\n\\n\\n\\n\\nLesson 7: Agent with Memory\\n\\n\\n\\n\\n\\n\\n\\n[Optional] Lesson 8: Intro to Deployment\\n\\n\\n\\n\\n\\n\\n\\nModule 1 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 2: State and Memory\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 2 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 2 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: State Schema\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: State Reducers\\n\\n\\n\\n\\n\\n\\n\\nLesson 3: Multiple Schemas\\n\\n\\n\\n\\n\\n\\n\\nLesson 4: Trim and Filter Messages\\n\\n\\n\\n\\n\\n\\n\\nLesson 5: Chatbot w/ Summarizing Messages and Memory\\n\\n\\n\\n\\n\\n\\n\\nLesson 6: Chatbot w/ Summarizing Messages and External Memory\\n\\n\\n\\n\\n\\n\\n\\nModule 2 Feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Module 3: UX and Human-in-the-Loop\\n                    \\n                      \\n\\n\\n\\n\\n\\n\\n\\n\\nModule 3 Introduction\\n\\n\\n\\n\\n\\n\\n\\nModule 3 Resources\\n\\n\\n\\n\\n\\n\\n\\nLesson 1: Streaming\\n\\n\\n\\n\\n\\n\\n\\nLesson 2: Breakpoints')]\n"
          ]
        }
      ],
      "source": [
        "### Langchain Blogs create a seperate vector db\n",
        "langchain_urls=[\n",
        "    \"https://python.langchain.com/docs/tutorials/\",\n",
        "    \"https://python.langchain.com/docs/tutorials/chatbot/\",\n",
        "    \"https://python.langchain.com/docs/how_to/qa_chat_history_how_to/\"\n",
        "]\n",
        "langchain_docs=[WebBaseLoader(url).load() for url in urls]\n",
        "langchain_docs\n",
        "\n",
        "doc_list_langchain=[item for sublist in langchain_docs for item in sublist]\n",
        "print(doc_list_langchain)\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
        "\n",
        "doc_splits_langchain=text_splitter.split_documents(doc_list_langchain)\n",
        "print(doc_splits_langchain[:2])\n",
        "\n",
        "vectorstore_langchain=FAISS.from_documents(\n",
        "    documents=doc_splits_langchain,\n",
        "    embedding=OpenAIEmbeddings()\n",
        ")\n",
        "retriever_langchain=vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "37839ec7",
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever_tool_langchain=create_retriever_tool(\n",
        "    retriever_langchain,\n",
        "    \"retriver_vector_langchain_blog\",\n",
        "    \"search and run information about langchain\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "60fee617",
      "metadata": {},
      "outputs": [],
      "source": [
        "tools=[retriever_tool,retriever_tool_langchain]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "630e7e2d",
      "metadata": {},
      "source": [
        "### Langgraph workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ed7232e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Annotated,Sequence\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    ### the add_message function defines how an update should be processed\n",
        "    # Default is to replace add_message says append\n",
        "    messages: Annotated[Sequence[BaseMessage],add_messages]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bc741722",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm doing well, thanks for asking. I'm a large language model, so I don't have feelings or emotions like humans do, but I'm always happy to chat and help with any questions or topics you'd like to discuss. How about you? How's your day going?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 39, 'total_tokens': 98, 'completion_time': 0.10349097, 'prompt_time': 0.001941281, 'queue_time': 0.055062378, 'total_time': 0.105432251}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_34d416ee39', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--fd434cc5-c72e-4ffb-8718-ef9b74d5d442-0', usage_metadata={'input_tokens': 39, 'output_tokens': 59, 'total_tokens': 98})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_groq\n",
        "llm_groq.invoke(\"hi how r u\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2432b11",
      "metadata": {},
      "outputs": [],
      "source": [
        "def agent(state):\n",
        "    ### DocString\n",
        "    \"\"\" \n",
        "        Invokes the agent model to generate a response based on the current state. Given the question, it will decide to\n",
        "        retrieve using the retirver tool or simply end\n",
        "        Args:\n",
        "            state(messages): Then current state\n",
        "        Returns:\n",
        "            dict: The updated state with the agent response appended to messages\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"--CALL AGENT ---\")\n",
        "    messages=state[\"messages\"]\n",
        "    model=ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "    model.bind_tools(tools)\n",
        "    response =model.invoke(messages)\n",
        "\n",
        "    ## We return a list because this will get added to the existing list\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "## Agent to tool connection is done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b5d87314",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing_extensions import Literal\n",
        "from pydantic import BaseModel,Field\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain import hub\n",
        "from langchain_core.messages import AIMessage,HumanMessage,SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "cf816d4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "## Edges\n",
        "def grade_documents(state)-> Literal[\"generate\",\"rewrite\"]:\n",
        "    \"\"\" determin wheter the retrieved documents are relavent to the question.\n",
        "        Args:\n",
        "            state (messages): The current state\n",
        "        Returns:\n",
        "            str: A decision for whether the documents are relavent or not\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"--- Check Relavence---\")\n",
        "\n",
        "    ## Data Model\n",
        "    class grade(BaseModel):\n",
        "        \"\"\" Binary score for relavance check.\"\"\"\n",
        "        binary_score:str = Field(description=\"Relavance score 'yes or 'no'\")\n",
        "\n",
        "    ## LLM\n",
        "    model=ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "\n",
        "    llm_with_tool=model.with_structured_output(grade)\n",
        "\n",
        "    ## Prompt\n",
        "    prompt=PromptTemplate(\n",
        "        template=\"\"\" you are a grader assessing relavance of a retrieved document to a user question \\n\n",
        "        Here is the retrieved docuemnts : \\n\\n {context}\\n\\n\n",
        "        Here is a user question {question} \\n\n",
        "        If the document contains keywords(s) or semantic meanign related to the user question grade it as relavant. \\n\n",
        "        Give a binary socre 'yes' or 'no' score to indicate whether the docuemnt is relavant to the question.\"\"\",\n",
        "        input_variables=[\"context\",\"question\"],\n",
        "    )\n",
        "    chain=prompt | llm_with_tool\n",
        "    messages=state[\"messages\"]\n",
        "    last_message=messages[-1]\n",
        "    question=messages[0].content\n",
        "    docs=last_message.content\n",
        "    scored_result=chain.invoke({\"question\":question,\"context\":docs})\n",
        "    score=scored_result.binary_score\n",
        "\n",
        "    if score==\"yes\":\n",
        "        print(\"--- DECISION: DOCS RELAVENT ---\")\n",
        "        return \"generate\"\n",
        "    else:\n",
        "        print(\"--DECISION : DOCS NOT RELAVENT---\")\n",
        "        print(score)\n",
        "        return \"rewrite\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8908e66",
      "metadata": {},
      "source": [
        "hub → refers to the LangChain Hub client, typically imported like this:\n",
        "\n",
        "from langchain import hub\n",
        "\n",
        "\n",
        ".pull(\"rlm/rag-prompt\") → downloads (or retrieves) a prompt template from the LangChain Hub that’s identified by the name \"rlm/rag-prompt\".\n",
        "\n",
        "So, the full line:\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "\n",
        "means:\n",
        "\n",
        "“Get the prompt template named rlm/rag-prompt from the LangChain Hub and store it in the variable prompt.”\n",
        "🧩 What \"rlm/rag-prompt\" is\n",
        "\n",
        "It’s a public prompt uploaded by someone (in this case, under the rlm namespace) that is designed for RAG (Retrieval-Augmented Generation) tasks — meaning it likely defines how the model should:\n",
        "\n",
        "Combine retrieved documents with a query,\n",
        "\n",
        "Format the context,\n",
        "\n",
        "And generate a coherent answer.\n",
        "\n",
        "You can inspect it manually by visiting:\n",
        "👉 https://smith.langchain.com/hub/rlm/rag-prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "cd966be1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "    Args: \n",
        "        state(messages): the current state\n",
        "    Returns:\n",
        "        dict: The updated message \n",
        "    \"\"\"\n",
        "    print(\"--- GENERATE ----\")\n",
        "    messages=state[\"messages\"]\n",
        "    question=messages[0].content\n",
        "    last_message=messages[-1]\n",
        "\n",
        "    docs=last_message.content\n",
        "\n",
        "    ## Prompt\n",
        "    prompt=hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "    ## LLM\n",
        "    llm=ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "    ### post processing\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "    \n",
        "    ## chain\n",
        "    rag_chain=prompt| llm| StrOutputParser()\n",
        "\n",
        "    ## Run\n",
        "    response=rag_chain.invoke({\"content\":docs,\"question\":question})\n",
        "    return {\"messages\":[response]}\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "3800cca8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def rewrite(state):\n",
        "    \"\"\" \n",
        "    Transform the query to produce a better question.\n",
        "    Args:\n",
        "        state(messages): The current state\n",
        "    Returns:\n",
        "        dict: The update state with re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"--- TRANSFORM QUERY---\")\n",
        "    messages=state[\"messages\"]\n",
        "    question=messages[0].content\n",
        "\n",
        "    msg=[\n",
        "        HumanMessage(content=f\"\"\" \\n\n",
        "        Look at the input and try to reason about the underlying semantic intent /meaning \\n\n",
        "        Here is the intial question:\n",
        "        \\n ----- \\n\n",
        "                     {question}\n",
        "        \\n-------\\n\n",
        "        Formulate an improved question:\"\"\",)\n",
        "    ]\n",
        "\n",
        "    ## Grader\n",
        "    model=ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "    response=model.invoke(msg)\n",
        "    return {\"messages\":[response]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "84dd9121",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARIAAAHICAIAAAAN8PI9AAAQAElEQVR4nOydB2AT1R/H313SPWnpoJRSWvaQUZAhe4PKEGQqQxCQIbL+OEAERGSLoiJDEAQBGUX2XmVvWmahdC/o3k2T+/+Sa0OapmmvI7m7/D7WcHnv3SW5e9977/d7735PyjAMQRCEC1KCIAhHUDYIwhmUDYJwBmWDIJxB2SAIZ1A2CMIZEcrmzpnkqBfZWel5slxFXg7DUIRS+dgZiqFpCvztFGzDP5BBCC1lFHmQr6AIDRmQriyjIOq9KJooIJFSJhJaVZAFsimKKNgjF3wEFKFVG4qCUrALUe1LqbKJ6pMK+/wl5kQilZhZUM4eFo1a27vWMCcIv6FEM25zaGNMTEgWSEVqTptbKGuhREJkOQqKKviNsEWpqjuj/J+t8bSUUuQVSItiCzFQy9V7UazSQFEKhkgoIs8/XRRsM6pE8qawqrzyVS0blRhVb5WHUBVWfQPNby61oBUKSpYpz8mRw/eXSGkHZ7NOA90861kQhJeIQTb7fomODc20sJb4NLHtOtiF0ETQBF5OfXApOflVLvyifuM9XWuaEYRnCFs2T26kndsbb+to1ndsNedqYqtehzfGhj5Od/eyGvxFdYLwCQHL5sjGuPDg9M6D3Ru8bUPEy9ZFYWChjV/iTRDeIFTZPLiUev1E4qffexMT4MTWVxEvMsYv9iYIPxCkbPb/FpUULRtnGpphOfPPq+f30yb+6EMQHiA88/nSgYTXkbkmpRmg23AXr3rWm78NJQgPEJ5sHgQkj19ci5gefca6w6AQ+NkJYmwEJpst34XVqGdNS4hpMvY777DHGUROEOMiJNk8vpaZnSHvN6EaMWGqVrfY+kMoQYyKkGRz7Vh8tVpWxLQZNrNGRnIeQYyKkGSTmZ73/niDNjUvXrx47733CHe+/PLLgwcPksqAIjb20kMbYwliPAQjm1N/x5mZSySGneX46NEjUibKvGNp8KxrHR2SSRDjIRjZxIZlO7pU1vSZtLS0FStW9O/fv0OHDhMnTvT394fE9evXL1y4MDY2tmXLljt27ICU3bt3T506tXPnzr169frqq68iIyPZ3Xft2gUp58+ff/vtt1euXAnlo6OjFy9eDCVJJdCqa1V5HgZOMSaCkU1WhrzyDBuQx4MHD0AJe/fubdy48dKlS+HtpEmTRo0a5e7ufuvWrZEjR967dw+k1bRpUxAGlE9MTJw3bx67u7m5eUZGBuy7aNGiIUOGXL58GRLnz58PQiKVgIMbDZ7okMBsghgJwTxvA/dXD5/Kks2dO3dAIW3atIHtadOmde/e3dHRUatMkyZN9uzZ4+XlJZUqT5pMJpsxY0ZKSoqDgwNFUdnZ2aNHj27VqhVk5eTkkEqGltDQT/NpYkkQYyAY2TAKYlOlsr5ts2bN/v777+Tk5BYtWrRt27ZBgwZFy0gkEuiVrVq1KigoCNoWNhHaHJANu92oUSNiKGiayUpDf5rREEwnjaEoCVVZ3/a7774bMWLE1atXZ86c2aNHj99//z0vT7tSXrhwAXIbNmy4cePGmzdvrlu3TqsAdNWIoVBaNhgX0ngI56FoRpGRnOtSo1K+sL29/SeffDJ27Nj79++fO3du8+bNdnZ2H330kWaZAwcOQKM0ZcoU9i14EYjxUMgpCxuMA2E0BNPa0DQVE5ZFKgGwT8BFBsYJmCggDLBYwBX25MmTosVcXV3Vb8+ePUuMB1h6bl6mPvJrRAQjG0sbSVRIpcgGTPwNGzbMnTsXmpqEhIQjR46AZkA/kAUOgNevX4NDLCwsrG7duteuXQOvGvTfWH80EBOjY2KlhYUFCExdmFQ0ORmMQqGo19KaIEZCMLJxqW6ZGCsjlYCNjQ14luPj48eNGwfDL9u2bfviiy8++OADyGrfvj3oZ/bs2SdOnJg8eXK7du3AvAGfAQzmgA8a7JzPP//8+PHjRY8JXT6wf2bNmpWVVfFSv3E8QSKhCGI8BPOYWnqSYuvikKmraxOTZ/uSMGt76aBpGGDAaAimtbGtQkvNqaNb8GkTkvxa1m2IG0GMh5C8MU07VLl3IUlPARikL85SBxuDHaYsCnifK2kWDKDnyHq+EoyrarofNDnwa5SFFe3ohm40YyKwWAJ/fBVSp5ld16EuOnOTkpKKsyVg5B4sdZ1ZTk5OlpaVNdweHR1dXJaer+Tm5gajqzqz1s14/sE0Tw8fnB9gTAQmm+gXuft/DTdZC+efHyOIhBo+x5MgRkVgD0V7+Jp71bPZsjCMmB43TiSlJstQM3xAeCE4+k2sBhbBPysiiSmR/pq5dSoRAz7xBKGGFzyyOS4hOnvU/JrEBHh2M/P07pjJK30Jwg8EHMx2x4/h2ZmKcYu8iajx/z0m5mXmZ8tRMzxC2KHTT26PD76X5lnbqv9nHkR03D+XcvXYaytr6ejvTKJRFRCCX6hDkUv++iE0M03u5G7W7j3Xmg3E4Jk9tf3Vi6A0hZxp0t6xwwBngvAMkSwLFfk09/z+mNREOUURS2va2kFqay+VmFGynEKh+ChaY8EmFTRNKRTqFZ2o/GWetIspV3aiaaJQvMliV3liE7V21zyCZiK7oFrB8lBvkJoTRR6dkZqXniLLTpfL5YyNnblvU9tOg1AwPEU8q6mxBF1NexmYnpIgk+XAL2NysgpXf1WVVa0/mD8VspBCqPyVCbVqtmqtNKogkQGZ0TTNHkSzpO7tgrUHNchft02NxEy5xJpEQlk7SKr7WmPzwn/EJpvK5syZMydPnly2bBlBTBic2sQNPRPJENMBawA3UDYIQdlwBWWDEJQNV2QymZkZLt1s6qBsuIGtDUJQNlxB2SAEZcMVlA1CUDZcQdsGIUJ83sa4YGuDEJQNV1A2CMFOGldQNghB2XAFZYMQlA1XQDboEkBQNtzA1gYhKBuuoGwQgrLhCsoGISgbrsBwJ8oGwRrADWxtEIKy4QrKBiEoG66gbBCCsuEKygYhKBuu4AxohKBsuIKtDUJQNlxxcnJC2SBYA7iRkpKSm5tLENMGZcMNaGrAvCGIaYOy4QbIBswbgpg2KBtuoGwQgrLhCsoGISgbrqBsEIKy4QqMdaJLAEHZcANbG4SgbLiCskEIyoYrKBuEoGy4grJBCMqGK+gSQAjKhivY2iAEZcMVlA1CUDZcQdkgBFcc4ArKBiHY2nAFZYMAFMMwBCmJd999NyYmBjYoimJTFAqFp6fnoUOHCGJ6YCetVIwYMQJczzRNUwXAdo8ePQhikqBsSsWQIUNq1KihmQJNDSQSxCRB2ZQKaGpGjhxpYWGhTmnbtq27uztBTBKUTWkZOHBg9erV2W0QzLBhwwhiqqBsODBq1Chra2vY8PPz8/b2JoipIkJP2v3zabGRWblZKjcx+L0YQtPg+FK9owhDKW8V7Ns36TSB06C09AmjlQVHUObK83e/c+duVlbWW02b2NrY0RJKIc8/e+AsUCgY9lXlMsg/Dntw1beApDenWiJVlmQUb7621FxiZSft3N+ZSAjCc0Qlmxd3s8/siYYKKjWjcrNYQagqLNT7Ag0oZUDy3xZKV78WzaLyE2GDUaIUCRwWTp5SheRN+YK9GIqmGA3ZKGGI5pmmJSpZaqRIzGAnKjdXUbWaxZCZ1QnCY8Qjm5dBWSf+jmnV07Wuny0RMnt/iqhaTfr+hGoE4SsikU1SHNm98sXIeb5EFBz4NcLGTjJomgdBeIlIXALHtkY6e1gTsdBjiGd8RBZB+IpIZJOeIvOoIx7Z2LooZyE8vJJGEF4ikqmcebmMhSVFRAT42VKT8TFSniIS2cjlijyZqDzpijyGkSsIwkvwwQEE4QzKBkE4IxLZUFTBYKVYoGiGkojrJ4kIbG34CkMRNG34ikhkw4itsVHOxMEHb3mLWDppBEEMh1ham8ITJcUAxVD4VAdfQduGrzAUg7YNX0HZIAhnxOOApkRm36C5xmNE09owhZ75Ej7KJ+BQOXxFNC4BimFEVcso5bOjqBueIhJnDZ9nCbx8+WLYiPcIVxgiOuegeBCPA5q3fbSnzx4RRFyIxbahOKvm6tVLZ8+deBB4NzU1pUH9xh9/PL55s5Zs1n+H9u3Zsz01LbVNm/bjxk6GtmLeN0u6de0FWQ8fPvhr24YnTx46OFZp26bD6FETbGxsIH3hoi8piurerc+Py7/Lysps2LDJpAnTGzRovGXr+m3bN0GBLt1a/vD9mrZtO5T2+1GqQB8ILxFJJ41WVjIOwsnOzl6ydF5OTs6Xcxf+sOQnLy/vb+bNSExMgKzHTx6u+Wlpp07dt/+1v3PH7ou+/0p5fFp5oiKjImb/b3J2Tva6X7YsXrgyJCR4xswJ7AIEUqn04aMHp04fXf/79mNHAizMLZYuWwDpY8dMGjZ0lJub+7kztzhoRoUCx234ikhkAzWMUXC4N1taWm7asGvWzG+ghYG/SRO/yMrKCgy6B1knTx52cnKG6u7g4NiuXcdWLduo9zp9+piZ1AwEAzLz9vaZPWt+8POnAZfPs7lZmZlzZn/rUa06SKhb194REWGZmZmkzDBEZL5BMSGe+Rtc3bWZmRm/rFsxeEhv6D71ebc9pCQnJ8FryMvn0LmCqs8W69ihm3qXhw/v16/fCOTEvnV3r+bh4QndPPZtDS9vNmYnYGtrB69paakEESMimiXA5dYcFxc7fcb4Fs3fnv/ND2CHgFnSo1d+q5Kenubq+iYmulokbNaTp49AZpqHSlJ17UhBRw4xBcQjG04dmvMXTuXm5oJhY2VlRQraGRYLC8s8jSXUExJfq7ednKs2adIM+m+ah3KwdySVgDJEJ8qQr4hlcg2tDA9besB7Zmdnz2oGuHDxjDqrevUawcFP1G8vF5gugK9PnZOnjjR9q4W6YQkNDfH09CKVgCpoLnrSeIpIbmiMgijkHMr7+NRJSHgNjmbwg12/ceXOnRvQGYuPj4Wsd9p1Cgt7ufOfrVBvb966Fhh4T73X4MEjFQrFut9WgSMOLP4/Nvz8yfihYAvp/yzQFXxWQMD5169fEQ4/CRwd6BLgKeKZJcDpzgyDMB9/NG7b9o1g0uzbt/Pzaf/r0b0vSGX1mh86dug6cMAQGJwZOKjHAf/d48dPJaploeDV3s5+86bdVpZWEz/7aNSYQffu354ze37dOvX1f1ab1u2bNG42f8Fs1lOHiACRxIBeN/N5y54ujdo6kHID7Q90vWrXrsu+hWGcyVNGb/xjpzrFMGxb+KJ5F4d271clCP8QSydNtWpGhQBtwqcTR6z9eVlsbMyjR4Fr1/7YqNFbvr51iGGB9hMnCfAWsbgEKi4GB4x+wjDoseP/fTJ+CAy/tPRrM2nSF5TBJ/GrFr9B3fAU8USuqcDO5nvvDoQ/YlQojFzDY/ChaAThjIiicuLgIGIo8HkbvkIR0YVHEA8iCsFBRAUjwjuBeMDwgjyFwilpPEY8c9JE19zg3Br+IhZPmjJujahqGTvcefbsWZlMlpOTk5mZCa/pKubOnUsQoyKacRuxhUeCbyZswgAAEABJREFUPufu3f8Gxe5nZQMQVc8NbJ6DBw9euXKFIMZDLFM5iQjt567dupiZmUHzAsqhVbCTFVAzRkcsc9LEaAY4O1edNm2avb29ZqKlpSVBjI1IZGNmQVmIqzqZWdBSC7pPnz79+/c3NzdXp0skkqVLl0ZGRhLEeIhFNubS+IhcIiLkckXNBrawMX369NatW7Pz00Azly5dqlu37tSpU2fNmnXvHj7AYxxEIhvP2lZRL8oRXYln3DqZaGZOu9XIb2TWrFnj4+OjUCjc3ZWxQQYNGuTv79+vX79169aNGTPm9OnTBDEsInlMDdj8bZiDo3mvcdWI8Nmx5OV746p51ivU7+zevXtRhTx8+HD79u1BQUEfffTRsGHDCGIQRCKbxMREqDQfd94gz6Wgb+Nc3UouzytajCrG30ZJJIxcVywCXTuAQ0uh80EF5Vz/Ik5wGHwpMmzJqBzJ2gUldHYGE/ooLSkm65NvfcxtOfjTY2Nj//77771793788cegHweHCnjKFdGDSGRz69Yt6MY4OTkd3RIfE5Ipy1Xk5eoIBVvcw2wUTTE6x+R1yYZSrnzOUEUUwlA6njClaKJrLUEdx6UllMSMtnOUjvi8BrEiZSAvLw9anh07dnTo0AHE4+vrS5DKQdiyefr0KRjHp06dIobi3LlzR48eXbFiBeExhw4dAvG4uLhA4/P2228TpKIRtkvg4sWL+/fvJwYEhk1cXV0Jv3n//fd37do1YsSIv/76a/jw4aBzglQogmxtQC3nz5//9ttvCVISwcHBYPZcuXKFNXsw4m6FIDzZQA9+7ty5y5cvh0EMYnAyMzOzsrKcnZ2JoEhKSgLxgOUDjhMQD/8bTJ4jJNmADWNlZdWuXTsj3jKPHDly48aNhQsXEmGyc+dO0E/z5s1HjhzZsGFDgpQJwTTZ4Cs7e/Zs+/btjdvNEIRtowcweMDU6dSp048//jhx4kTo7hKEOwJobaCR6dGjBwxNsGPkSEVx+/ZtcLiFhYVBt23gQCMHuBIWfJfNpk2bIiMjv/vuO8IP0tPTwbhydKyUxTmMAsgGum0nT578SIV6FQZED/yVzd27d6EL/vDhw0aNGhHesHv37vDw8Dlz5hBxkZGR8beKvn37gs/N09OTIMXDU9tm2rRpERERsMErzQA2NjaCc6OVBvhdYOrg9OpSwrvWJi4uzt7eHq5Z27ZtCWIkLly4AN5q6I5Ct6179+4EKQyPZJOTk/P555/DmIyPjw/hK2lpaQqFwkTmSuL06uLgkWyOHTsGvl0/Pz/CY/7888/s7OzJkycTkwGnVxfF+LZNYmIidKZho0+fPjzXDFGunG7r5ORETAnw+8+ePTsgIMDa2nrQoEEw1PvixQti2hi/tZk/fz50APhm+iPFgdOriRFlA46y06dPjx07lgiKlJQUmqbt7OyIaXP16lXouUFPAcQDPmtiYhhHNmD9Qwvz+++/C27g/5dffoHO/ahRowhiwtOrDf07oVsM/hnQ6oEDB4Q4WQac42gTq6lTpw6YOnv27IFGuE2bNqtXr46PjycmgEFbGxDM4sWLt27dijHyRInpTK82kGyeP39eu3btR48eCf1sJicnS6VS8KcRpBhOnjwJ4rGysgLxdOzYkYgRQ8hm//79p06dAkuGCJ8ff/wR9D948GCC6EXc06sr17aJiYkhqrEOcWgGcFBBkJKAIbjVKqCL0alTp40bN2ZlZRGxUImtDZwy1rtPENNGfNOrK0U2cJpyc3OPHTs2YsQIIi5gpAL8GTBeThDu7Nu3b/v27b6+viCeZs2aEcFS8bJZtmzZoEGDfHx8+OnFl8lk2dnZpKxcvHgRbpblmWxqY2Nj4uFjRDC9uoJlA9a/XC7/8MMPCV+BlrA8nez09HRzFaSsgGlkZmZGTB5BT6+uMNmsXbt2+vTp0DcrT5UyAOWUTflB2Wgi0OnVFdNb+Oyzzxo0aAAbPNdM+VEoFKJZo4EPCHR6dXlbmxMnTvTq1SsnJ8fCwoIIgXK2NikpKTCQh520SkIo06vL3tqAYd26dWtvb2/YFopmyo963VlNhg4dunPnToKUG6FEry6LbKCBioqKgnv2lStX6tWrRwTOkiVLoM0sZWE7OztsKyqbtm3b/vrrr4sWLbp+/XqPHj22bdsGfWPCJzjLJiwsDFpPqD1VqlQxShTmCic4OLj0hdG2MRh8nl7NwbZhvWRgvbVv354IFi3bpnfv3uwGDKfAYBwpeAArIiLC3t4eBuamTJmijl4LWdB5iI6O1sqCTlr//v2hawEn09/f/9SpU9Aa16hRw8/Pb9SoUVo3F7RtygavpleXtrW5fPkyO+QvaM0U5eDBg/A6Y8YMVjN37txZvHgxjMHBkMLXX38Nt7d169axJdmsTp06bdmyRStL82jQNR84cCCo69133z1+/Pi///5LkIqAV9GrS5YNNDJENTgFznUidqAb/c4770C9hzYBbmkTJky4cePGs2fP1Flgpzo7O2tlqQkMDISuBXTHHR0d+/Tps2bNmlatWhGk4ujZsydcCDj50KqDw/rAgQPEGJQgG7CVt27dChvwRYkJ8PLlS00nR926dYlqqUN1ltq20cxSA3K6e/cu9MJPnjyZmprq4eGBC2hWBkWnV8t1rlhcaeiTDYzgXrt2zUQEQ1Rmj9YAFBtHPDMzU52Vnp7OXiF1luYRoJmaOnVqcnIyXFHoVCxfvjwhIYEglUPNmjW/+eYb6LnBCQe3GzEgUj15MIK7YMECYjKwgtGc6MmqwsnJSU+W5hFgVKePCvA33rt3D0xY0Jtw15ASBODLgRO+atUqYkD0tTaRkZFg0hCTQSqVgmXy+PFjdQr0AeC1Vq1a6izwocG2ZpbmEcCHFhoaSlQ3QvCtDRgwACPxiRJ9srl165axTC6DAc1I1apVb9++ff/+/by8vH79+sEYLpibaWlpkLJhw4ZmzZrVrl0bSrJZ+/fvB6NFK0vN+fPnwdsGPVsoAw4DcD/iQn+iRF8nzdPTk2+js5XBsGHDwN0M9whw0YDrGawR8BmuX78exmRatGihDoCozgLBaGWpmT59OuzILmIFw8HQeQBvD0FEhyAXWC8P5ZzKCa2QpaVlecYrcbizwgkKCgLbBsbTiKFA24YbOCcNIWjbcAXnpCEEbRuuwLgNdNJE/zQeoh99smmpgiAamHj0DIQFbRtu2NraYlODoG3DDbRtEGKCto21tXV5HuFesWJFq1atOnfuTMoKdvNEgMnZNhRFsbNjygbrgC7PERARoO/yg22TkpKCq2pqwq7Oi5g4aNtwIyEhIS0tjSCmjT7ZgG2DMxG12LhxY+nD3CBiBcdtuOHi4oJLqSFo23Bj3LhxBDF50LbhRmJiYmpqKkFMG5yTxo0dO3Y4ODiMGjWKICYM2jbccHZ2xtXhEbRtuCG+ZRWRMoC2DTfgPpKcnEwQ0wZtG27s27cvOzt78uTJBDFh0LbhRpUqVYy7hiHCB9C24cbAgQMJYvKgbcMNGLSBoRuCmDY4J40bx44d27x5M0FMG7RtuAG2Dc4SQNC2KRW9e/eOj4+nVIB3cf369QzDwNDnqVOnCGJ6oG1TKoYMGSKVStk1otWLRWNTbLKgbVMqQDY1atTQTPHw8MAZAyaLPtnA3fSDDz4giCrOU79+/TRjd8ANpUmTJgQxSTBOWmkZNmyYl5cXu121atWhQ4cSxFRB26a0QFMzaNAga2trompq/Pz8CGKqCH5OWvjjnMzMHMJ+TTDUGY0NMNwZJv+1UC5FMewmo0oueEep/i+IHqgspY4kSNNEoWhcs2ez2i9S09K6tBr05GaBG5pSHUEj5uCbA2qkwHEVhVJUJWiKKJjCSYWQmElr1rM2tyIIrxDwuM2/P0W+js6F+ijLVVBskq6apwNVaaWsmJKKqat0gYTqOgxiHEjYTRJ2I15nyWJh8j+3NB+nRmpOKxSMlY1k2AxvKweC8AShjtv8syJSlsP0+cTTuZr4IzJf2he/dUnImG9qWjlICMIDBGnbbF8SThNq4LQapqAZoMMg1+Ff+mxZEkoQfiC8cZuQwKyMVFnfCdWJKSGRECc3y39WRBCEBwhv3CbwcoqVrSkuA1iroX16Yh5BeIDwxm2y02SmGbPfzkmaJ8eHbXmB8Gyb7By5TGaKK8zAYIDcJH84D8FYAgjCGeGN21C0arTS9GCHZhE+IDzbhlGuAmia1Ych2EfjBzgnTTBQ2NbwBuHZNrSEmKjBRWEnjS8Iz7ZRTn00ybWacYFq/iDA520UxERtGwYbG76Ato1goChscPgCjtsIBoqgbcMXME6aYMCWhj8Iz7aRSJSxyogpYqLDVTxEeLaNXA7DncaZy9l/YLdt2zcRo0GhbcMTME5aIV6+fDFsxHvF5Q4d8vFbTZoTI4LNDT9A26YQT5890pM7YvgYgiAmEicNOlf79v0zfcanXbq1TE1TRpw5fuLQ5Klj+rzbHl737tvJhtfYsnX9suUL4+Jiodi/e3eEhDyHjWvXAgYP6T1+wnBSuJP28OGD/82d2q9/l49Hf/Db72syMjIg8eata7BLUNB99Uc/fvJQeZDrl4vbpfRgF40/CM+2kUgIRXNzCZiZmR0+eqB27Xorlv9qbWV9+sxxkEfdOvV3/v3f+HFTQDbrflsFxcaOmTRs6Cg3N/dzZ259OHgk7AWJ2/7eBH2zWTPnaR4wMipi9v8mZ+dkr/tly+KFK0NCgmfMnJCXl9eieSs7W7uLl86qSwYEnIOUVi3bFLcLKTWMcuCGIHxAeLaNXE4YBTeXALje7O0dpk2Z3dKvtVQqPXrU/623mn8x/csqVZygoo8dPcnff09SUmLRveAVajxIqEH9QuF7Tp8+ZiY1g9rv5eXt7e0ze9b84OdPAy6fl0gkXbr0vHjpjLokSKhbt96QXtwuBBEgphIDul7dfP3DAG7Qw/utWrZVZzVv3goSHwTe1blj3ToNiiY+fHi/fv1GDg6O7Ft392oeHp7sETp37gHdvGfBT4jKwRAZGd6ta2/9uyDlRzM8twEQYJy0MnmTzM3zQ0Pl5ubKZLLNf/4Gf5oFirY2+Tvquh7p6WlPnj4Co6XQERIT4LVZUz9oxC5ePAOdwEsB51xcXBs3bqp/l9KCM6CLJycnhxgQfbIB2yYoKIh3silf/97S0tLa2rpnj3c7duymme5RzbP0B3FyrtqkSTOwhTQTHeyVLQl07aCfBr0vsJrAsOnRvW+Ju5QWBh9T4wuCfN6GKd+X8vWtm5ae1rxZ/o0fGp+YmChXVzcOR/Cpc/LUkaZvtaALguiEhoZ4euavR9C1c8/9+3eBCw6sl6+/WlyaXUoHtjV8QXi2jUJe3sfUPh039fLl80ePHYSbQmDgvUWLv5o5exJ03ojyTuGVkPA6IOB8RESYniMMHjwS9gX/W3Z2NpT8Y8PPn4wfGvLyOZvbqNFbIEJwZ/v41AbrvzS7IMLCFNe3gc7ShvU7Hjy4O3BQD3AKZ5xDSSUAABAASURBVGSkf794NWtTtmndvknjZvMXzD5z9oSeI9jb2W/etNvK0mriZx+NGjPo3v3bc2bPB2NGXaBzpx7gFejapVfpd0EERKEVJrTw9/cH22bevHmET/z1fag8j/pwRk1iYoQ9yji/J2bqmtoEKQzU0lWrVm3ZsoUYCuHZNpSpOpQofC6aNwhvTpqy6phk7WHXuUL4AK7diSCcwVgCCMIZjJOGIJwRoG2jMFXTGCfX8AYBxoBmTDROmkKBk2v4gvBsG5NdcYBGPxpvEJ5tY8IrDiB8QXi2DU0ThcmqBm8X/EB4to1CYcKD5Wjb8AMct0EQzmAMaAThjPBsGwsLiUxiip0VSkJLzNC44QXCs21sHKSmuc54Sly2RGqcKL6IFsKzbVr3cc5KlxPTI+RRRhVXg8ZnQYpDeHHSXGuYQ+3ZvzaCmBKRT3MzEnM//MKDIDxAkHHShs2u7uxhtmd12NPraUTsvI7OPbY1+sLeyInLfAjCDwQYJ03Fe+Pdj/4Ze/f86xsn4xVyBddlLWHkR3OqCqV3RIRRLiyj+/jF7Vh4wUCG2zilRnEJrfQE2FeRTlqOmuERAoyTVkDfT9yV/8hJVpacaBo7VEHNU1dc5cwCRaEU9Ta7QVOqFajV+76p9afPnLl//96smbMKHUoNrVyCt+CY9JtQVBKJMupufhmNg1MF5SnVJ6m/p+Y3pKiMjPRhQ4f+uWWLi5u7lS1B+Ibwx20kxMpWUqpyZeJR8N1GzepaOUhKd6hSFithXyt7+yOn/C9duuTl604Q/qEvcg3CByZNmvTTTz9ZWloSpBgMH7kGYwmUQHp6OjEqn3/++dKlSwnCJ3BOmj6uX78+d+5cYlRgDGDhwoWwsWfPHoLwA1y7Ux/Pnz9/5513CD+oXr366NGjCcIDcO1OfYwcOZLwBhBw7drKkJyPHz9u0KABQYwH2jb6CAsL45XLxM1NuSxCUlLSzJkzCWI80LYpFuihgWFD8e8J/nbt2g0YMABualwXzUUqCrRtiiUiIqJXr16El3Ts2BGuDijnjz/+IIjBQdumWLp06UL4Tb169S5evBgQENC+fXuCGBC0bYrlwYMHBl4Rsgx8+umnjRs3lsvlZ8+eJYihQNtGNykpKWB2G3j94bLh6OgokUhOnDhx9OhRghgEtG10ExUVNWLECCIcli1b5u6unMAWFxdHkEoGbRvdNFRBBEWLFi3gdfXq1WCV9e7dmyCVBto2url27RoMjxABAs0ONJUEqUzQttHN559/7uDgQITJuHHj4PX333+/f/8+QSoBtG10EB0dPWXKFJoWdpiY8ePH//zzz9nZ2QSpaPB5G5EDPvRHjx7VrVvXxsaGiBR83oYXXLp0KSwsjIgC8KH7+vr27dv31atXBKkg0LbRAVjVYnqa0t7e/sKFC/Hx8VlZWQSpCNC20SYjI2P48OHsXGMx0ahRI7DWBgwYACO5BCkfgoyTVqmADcCrx2wqEOiw/frrr/7+/gQpH2jbaHP27Nl79+4RkaJ+RHTFihUEKSv6ZAMOCugTExPj/PnzphAmplOnTj/88AMRBdD59PLyIgZE3+SaqlWrZmZmEhOjc+fOLi4uROy8/fbbzZs3h42XL1/WqlWLCJlnz56ZmZkRA4K2jTZdu3Z1dnYmJgBb1fbu3XvlyhUiZJ4/f85GWTAYaNtos2vXrvDwcGIyzJkzR+hXOTg4uE6dOsSA4LiNNgEBAdHR0cSU+PTTT+F1x44dRJjwq7UxzXGbYcOGGdi+5An169dfsGABERqvXr0yNzc38LxbfN5GG5N9Lt/Pz8/R0ZGoBnwFNIHN8D00grZNUf7777+nT58Sk8TX1xde165dC74pIhCgh8Yv2ZimbXPjxg3wyRIT5uuvv968eTMRCCAbVu2GBG0bbfr16we9fGLaLFu2jKhGfgnvMUonDZ+3QYrlwoUL0OOYNWsW4TGtWrW6efMmMSxCXbuz8jhx4oSrqys7gm7idOrUSS7n9Vr2L168MHwPjaBtU5TAwECTdQkUpWvXrkTlJODnszrQQzPwiA2L8NfurGh69eoliKiChmTChAlDhw4FHyPhGYYf6GRB2wbhALTD9erVI7xh+vTpQ4YMMfzSXThuo83FixeFPrWx8oBOEa/m4BirtUHbRhsY6Xvw4AFBdPHee+/xZ1GdtLS0zMxMozy+jraNNh06dJDJZAQpBrBz4HX//v1Gf6jEKCM2LDgnLZ8ePXokJCSw2xSVb/I5OjriAhg6adOmDYwLazoJOnfuDJbGwIEDiaEwyrQaFrRt8unYsSNIhVYBsmFDcpr4ir968PDw2LRpE1EtJAqvvXv3hi4TNEHEgBhlWg0L2jb5jBo1ysfHRzMFBj3B60qQYoDzA6+nT5/u2bPn69ev4V4TExNjyAF7I3bScE5aPjVr1mzfvr3mArdwSfz8/Aiil927dycmJrLb0PIcPnyYGAqeysbUYgkMGzYMxMNuOzg4wIAAQUoiJCREvQ03ndu3bxtmXaqoqKiqVasaK8YQ2jZvqFatWpcuXdgGx8vLC1xqBNFL0dY4Pj7eMA2OsabVsKBtU4jhw4eDYKytraHlIUhJjB07tmnTpu7u7vb29goVeXl5J06cIJWPEXtoRP/kGpBNeHi4Iftp144mPbyWkpulyJMriJEm/cDHUsRoSKSUVEJX9bT4YKoH4Tcv72Vd8I/PypAr5IwivxapT17+BsUQpuBsUiT/kkKVU9uQ6sRCaFwDzSPoR/Ow2kcp5rOKpqjOP+XsYTHo8+qkeHg0J+3qoaSH15NrNnBs1MZeCl1W1Yx1BrzBCtU3VF0Fwn5bSvV72S+uvkz6tlVvmILjMKoUzR+u+ZamlAXUb6mC8695njS/DCnycRowNKGKjhgXUxiQSCQvn6Q/vpKUxzBjv+VvJJCwJ5nHt8ZVr23T6B1He3vz/OcLJDSRq36thCJy1c+jVT9TdcKh9isvJZxq+Pnqawpv2UT2LDPq7YLL9CZdtUlTBftq1AH2rfqwRKm0fMVoFMj/AkRzF4YUvjpw/kOfZjy+lpiVkffpkmKjLuqTjSGftzm941VIUMbwL70JouL8roT4yLRxi70J/wi6kh7gHz/yGx8iXs7tfJ0Qnz52gbfOXL7YNs/upQ2a6k2QAjoPc6Yl1LEt8YR/XDnyqnknkcf77TKiKrRZxZ1/XsxJu7AvwdySNrcliCZuNW1iQ9MJzwgJzAJjpmF7OyJ2qtWyiX6h+/zzYk5a8us8iVTY68tWBg5VzSOe8W4qbUJsTmHLW7TYOJnJnug+/7wYt5Fly3Kz8whSGJksV5bDu4cIZbl5shyTmBcvl8mKO/84bsNnTOS2LjzweRteg0+s8xN83obHUAzB1saIwLhOMb0xnJPGYxjsoxkVGE0tprOFtg2fwbBCPIUXtg3cVGm8sRaBYiiaf2eFQj8FT2wbuKcq8L5aBIZiFPw7Kww2gTyxbcDwwjuYLvDGbkyU7SrF43EbMLzwDqYLvLEbE9XZ133fwnEbHgMGH/+mHDGm1AAWd9vCcRseAwYf/+5aFDaAOG7Dc9C2MSJKBy/N5zlpFI6G64aHtg3frtW+/bu69XibVAJKB69C94/lR5w0hoij5T/gv2fpsgWkwuCjJ41v16phg8YffzSe3a7o818saNtUJE+fPiIVB5X/GD6ijwYNGsMfu12x518PQl27MykpcemP3z589MCrhnf//h9GRoZfCjj315a9kJWXl7f5z9+uXQ+Ij49t3LjZwP5D2rRpz+414IPuY8dMSklJ/mvbBisrq1Yt206dMtvZuSpkJSYm/Pb76qCH97Ozs1u1ajvqo/E1atQkyvB5z8d9Omzpkp9Wrv7e0bHKpg3/vHz54r9De+/cvRkbG+1d06dv3wH9+w2Gkl/MnHD//h3YOHnyyB/r/65bp/7Dhw/gg548eejgWKVtmw6jR02wsbEp/W9klJEexNB7XfDd/yQSiZtbtV27ty38bnnHDl11npn/Du379bdVRw5dlEqV1XL1mh8OHd7/56bdtWopAz1D7u/r1xw6eH7Qh73g6lwMOPvgwd2D/mdPnToKF+7MqRsVfv71wAvbhqI5z9hYvnJReEToiuW/fb949fXrl+GPDXYO/PzL8r37dg4cMHTnjkOdOnZbsPB/Fy6eYbPMzMx2794GJf0PnPlry77AoHtb//oD0uVy+YxZE+/dvz3ji6/hOlVxdJo8ZXRUdCS7C7xu+3vT0CEfz5o5D7bh0t68eXX653N/XPozaGbtz8uuXb8M6T+t3gC3vZ493z135hZcs8ioiNn/m5ydk73uly2LF64MCQmeMXMCSJpwQCRNDZzDkJfP4W/J4tVvNWle3Jnx82udm5sbHPyE3QuujpubO9wZ2bdwR2vp1wYUBUc7fPRA7dr1Viz/1drKWv0pFX7+GeWcL+4uAcPZNhQ3t2ZKasq1awFDPvwY+rXQVkBthhs/m5WTk3Pi5OERw8f0e3+Qg71D3z79u3XtvW37RvW+1avX+GjkJ3a2drAjtDbPnj0mymVu74WHh3791eLWb7dzcnL+bNIX9g6O+/btJAXmRauWbT4cPLJBfWXDO3/+0hUrfmvRvFXzZi2hnalXt8GNmzpWXzt9+piZ1AwumJeXt7e3z+xZ84OfPw24fJ4IHIr7pDTYAS7QwgXL27XrCC12cWemuoenWifQmwgLe9mzx7sPAu+yBwkKvNeixdvs0eztHaZNmd3SrzXbLumk/OdfGQ2K4e4SMFgMaEbOcIomGBEeCq+NGzdl39ra2rInlCjXQnsMdyzQg7pws6Z+0NECpbFv69ZtoM6ys7PPyFDGWIAbG9zDQAlsOlwY2Ov+gzvqknXrNND4usz+/btGjRnUpVtL+Hvy9FFyUmLRL/nw4f369Rs5ODiyb93dq3l4eKorQWkoWC6Eh3BuBmt61VLHa9ZzZvxatA4Kug8b8LZO7XrNm7d69FCpolev4mNio0En7C716pZ8Ny//+dfj/xekbZOuqus2Nm9C3cDtJz8rPQ1ep00fp7VLUmKCg6qMzlsl7CWTyUADmolwX1RvmxesHa1QKL78erpMlvvp+KnNmrWEVqvoZ6mPCYrSOiZ8DVJqGGWwSx7205gyWFzmGotv6zkzoJNf1q2Ajfv3bzdp0rxhgyaxcTGgGeg/u7q6sdam8mjm5iV+YvnPvx7/vz7ZgG0TFBTEQ9mwC6DLcnPVKUnJ+fd756rK+F2zZn4DnTHNXVxd3fUcEDps4CFY8v0azUQJLSla8lnwEzAxV674za+gfYPL41LVtWhJJ+eqTZo0Aw+EZqKDvSPhAg9dAuUXsp4zA86Y1NQUaFigWRj18adwoevVawh9gaCgey2acxucqYjzX+zJ58fzNoTbEFo1d2V83pehL6DPSpQVN/3OnRvgqIFtz+perKjA8GALQy8Zbo/W1tZ6DujrWzcrKwukBd1rNiU6JsrRoUrRkuCFg1e1TkJDQ+CvlreONb2VLcBQAAAQAElEQVR8feqcPHWk6Vst1D0tKOnpySk+rTgHgfWcGegR1Pate+XyhRcvgqEApDRp3Cww8O7tOze0BFCeTyk1TFlmQBtufRuK2zQS6KfWrFkLfIvg7ALN/LR2abVq+YGuQR5jRk8EHwBY+WDkgA8N3Ck/rf1R/wGh6Xj77XYrVy6Oi4sFYfgf/HfSZx8fP/5f0ZLgcQYzdPee7alpqeBFgB4FeAugI8HmQhP3+HEQ+KZBq4MHj4SbzrrfVoFHOyIi7I8NP38yfih4kwgH+OhJK4NLQAv9Zwb6afsP7IIbImuWNG7UFNykUVERasNGDxV9/ott7XkxJw3afa6d+P/N/hbuIh+PGgheRbDy4eSC24TNGjZ01JzZ3+7ctfX9/p3BO+xRzXPWrHklHhBGZjp16r7o+69gbAcuW/fufT74QMdaHeDq+ebr7x89Duw/oOvX82aMHzelX7/BcKlGj1UO3bz/7gdQp+b8b8qLkGB7O/vNm3ZbWVpN/Owj8B9A73zO7PngGCUc4GNrw5T7OTX9ZwYcM9DUg5+afQsdLeizgXtAbdzroaLPf7HoC53u7+8Pts28eSXXuXKyd21kQmzuiC85hOKGNgHuIlCJ2bdfffOFVCJdvGglERF3zrwODEieutpoix/p5MqR13fOpIxeYJy1Zg3J7TOvgwJSpq7W8Uv5YdvQnGdfLVz0JQwFfPbZDLgtwfjx7dvXtQx6EaBenwThG/yYk8ZwniC4YMGyFSsXbdy07tWrOBgTWDD/R7AxCFL5lN+2EQp6Hhzgx7gNxdn6BZfL94tWEcTgMCYTg6OMDw4YMpYAIhSUa5CZTM+xuF8qyHEbxIio3J7EROB3LAEadaMD5ZQ0mnc11HRsGz3wY9wGAz7pglGuWM7DyTWmYtuonmfB9W0QhAuqmbQ8jpOGto1OGDwtfIUvsQSwj1YUipenBU0bwhfbhiCCAUOnE7RtEKQM8MK2kUop+CNIYSQSqdSMd6dFQkt4+K0qAz3nnxe2jY2DRUI8LrCuTXa6wsyMd8EEbB3NTWSaQG62QirVff55Ydu06lE1Nwtlo018eIaDixnhGY3a2sCAUuzLLCJ2Yl5kOLrqPv+8sG0c3YiDs8X+nyMJUkBKEklLyhs8vTrhH151bC7siSOiJj0Z/oo9//oeUwPZhIeHG+i5aEL+XROVl830HONpbk1MnCsHE0KCksd862tlS/jJxX2vQwIzOw52d6lRchAZwXHtUGLwg6Tx3/sWFyGH4pU78d/VUQkxOZSUyGVFlnZRjWIwlPI/9dtCQxs0QxTKMYX8H6TO0ipZZMc3u2i+LSigzmU/urhcVYGCIImqXK3DqsZ0878h0ZwjWHh0RgLXSU6ZW9MffeNtzu8KeXhTbGRwJvsz4XppZlE0XL7C9g9NCkW01n7LsKvJF1w7hp18rPm24PIp8rtIDGHPMltMGUGzIE15KWgqv/6w34JR7kTlT0ItyFVN5WaLQZVjVFaCxJwicsbMSjJslreNQ3E/Xa9sjBUn7d7F1Oy0PC0nnnJ6EMPQNKVgl4GlKQIbRWq/urJqbajOVJGarrGj+lBx8XHhYRGtWrXU/Fx1MfZj1QfRPJhaD+wuRdRIqdZ5oggrPYZoHZ/F3JL2aVzFqZpgbO7HV9NTEnMUhdfmpSVEIS9UTLnmtUbECEq1Mrh6r4JrV3AqKFIgm/y3yqz8687em1QlVcek2dNKMQwrESb/0QZl9aEYVfH8mpMfskIVOZmRK5TFaFohV1UziVIt8K+ZBe3bpOTzz8c4ac062hPjcfbsg6inZ6e/35sgpaBBW+hH8rUrWWng2p3a5OXl6YksjCAE17cpCsiGXWUAQYoD1+7UBlsbpERwTpo2KBukRNC20QZlg5QI2jbayGQylA2iH7RttMHWBikRtG20QdkgJYK2jTYoG6RE0LbRBmSjfw0pBEHbRhtwCeBwJ6IftG20wU4aUiJo22iDskFKBG0bbVA2SImgbaMN2jZIiaBtow22NkiJoG2jDcoGKRG0bbRB2SAlgraNNigbpETQttEGn+5ESgRtG21ANhKJhCBI8aBtow120pAS0ddJe/r0aXh4ODExPDw8QDkEQYpHn2zq1av3559/Hj58mJgMv/76q4+Pj5+fH0GQ4ik5mG1KSoqlpaWFhQURO9u3b09MTJw+fTpBEL2UvHyKg4PDtWvXIiIiiKg5ePBgaGgoagYpDaVadahTp05r1669evUqESnnzp0LCAiYP38+QZBSwK8VB4zC7du3N2zY8McffxAEKR3c1riD3n9kpKgWbwoODl65ciVqBuEE59Zm7ty5EydOBHcTET5xcXGffPLJkSNHCIJwwXQ7aVlZWT179rx06RJBEI6UcSHiRYsWJSQkECHTtWvXs2fPEgThThll8+23365ZsyY1NZUIk169esEwLk7ZRMqGKXbSPvzww+XLl9eqVYsgSJkoY2ujZuTIkbm5uUQ4jBs3bt68eagZpDyUVzY7duxYtWoVEQgzZswYM2ZM06ZNCYKUAxPqpC1YsKB169Z9+/YlCFI+ytvasGRmZvbo0YPwGGgS69evj5pBKoSKkY21tTUMGu7Zs4fwko0bN9ra2g4fPpwgSEVQkZ00hUKRmJhYtWpVwid2794dHh4+Z84cgiAVRMW0NvnHoumcnJz+/furU9577z0wwYlhgTEZ9faxY8eCgoJQM0jFUpGyAapXr75z585r167BNugnNjY2Pj4+ODiYGIpt27ZBi9eiRQvYvnz58vHjxxcvXkwQpEKp+FgTNjY24OEF4xsEA29fvXp1/fr1OnXqEINw8eJFuVwO7V7Lli3NzMxE/IwQYkQquLVhGTJkCKsZACox3PWJQYiOjo6LiwPNsG9lMlm/fv0IglQ0FS8b6JvFxMS8+QCajoyMNMwz1YGBgVoTTEFI6HRGKpyKlw340yiK0oxLCC3AjRs3SOUTEBAAPgnNb+Lg4ODq6koQpEKpeNvm0KFDMIBz8uRJtsvEqIB+2qBBg0hlAl2yBw8esIq1tLR0c3Nr27YteNVwKg1S4ZRr3Obh1bRnd1ITYmV5uXJoXZRHetPGMIQiqiZHwSgIRVMqk4MhDKX8D/KgMM0QBcV+C0hXbrxJUZYt2GBU/2h8aZowCtVHFKRDikLBMPB5oBvVASnyJiAtLaUhhaaIuaXExdP8rfaOXvWtCIKUlbLIRp5L9v4alRCdzTCU1FxiZiaxsDWXWtCMssbLlZpQqoBiKzxFqWo9o1IBm0VUNZv9V/Xp6urPgLyYQlGnGZVu1KJRHhZ2oWmVItXaUmqNVh4l/4CgUUYzerUE0mlZVl5uZm5uNohcIZFSnrWt+02sRhCEO5xls2tFxOvYXAtrM1fvKg4e1kSYxD1PSYpKgUbSqx6Ix4MgCBc4yCbqabb/xigQTO221YkoyErODb0XA03UpB/FEFEEMRillc3144k3TyV6NnR19LAh4iLmUWJidMpHX9dycMb1OZBSUSrZPL2dcWpnTOPuon0iMi+beRIQNvobbzsnVA5SMiXL5tqhpLuXkhp0qUnEzqMzoR/P9bFzoQiC6KWE4c70FPmtcwmmoBnAo7H79hUvCYKURAmy2bEszMW7CjENHN0sLaykf34XShBEL/pkc3hzrEJO3Oo4EpPBt031rLS8x9fTCIIUjz7ZhD5Mr17fhZgYDq62l/xfEQQpnmJlc+afVxIz2t6dpwOa9wJPz57fOj0jiVQ0nm+55MmYyGfZBEGKoVjZvHiQblNFqJMAyonUUnrhADY4SLEUK5uc7Dz3uk7EJHFwsUl+JaRQo4iB0f3gwL0LKTCkY25VWWN/oeEPTp7bFBH5yNamSoN67Xt2GW9pqZx8cPnav6cu/PnZJ79v2/VVXHxINbfaHdsNb9XiPXavw8d/uXX/qIW5dfO3erlW9SKVhludKvGhyQRBikF3axP+NEtqVinPSwOvEyL+2DpNJsuZOmHT6BHLYuKCf//zM7k8D7IkUrOsrDT/IyuHDPh6xaJrbzXuusf/+6TkWMi6cmPflRt7P3h3zvSJW5yreJw6t5lUHpTy+YOH19CfhuhGtzay0+W0ecU/wcZy5/5xqcRszPBlbi7e7q4+H/b/JirmadDjC2yuXC7r0WV8zRpNoOK2bPYuwzBRMc8gPeDqnrcadQMhWVvbQ/tT26clqUxoCfU6KocgiC50y0Ymy6MqLTY09NBqeDa0sckfDnKqUs3ZyfNl2D11Aa/qjdgNayt7eM3KTgPxvE6McHN9MynO06M+qUwommSl5xEE0YXuJoWmaIaqLNlkZadHRD0C97FmYmram9AZ7AOammTnZCgUcguLN549c/PKfTwTumkSKU5OQ3SjWzbmVjSdUln3Wjs751o1m/XqOkEz0cbGQc8ulhY2NC2Ryd6MpeTkZpJKhWFsHHCtNUQ3umVTxdXidZSMVA4ebnVu3z/q491cHdAsNj7ExVmfZwzanyqO1ULDAzu9k5/y+Gnlxl6TKxhPXxMdtkJKRLdtU7e5bZ5MTioH8CkrFIr/jq3Jzc2OfxV2+MS6VetGxMQ9179X08bdAx+duxd4GrbPXtoWFhlEKo2sZOUtw6uBJUEQXeiWjWddS3AlpcZWSkcIXGGzp+40N7P6af3o5T8PCQm98+GAb0o08bt3Gtvar7//0VVgFEFT06/PF0TZk6oUA+x1eIqFZWX53xERUOxjan8vjcjJoX1buxPT48n58Jr1rfuMdSMIooti76mtezllp5vidEZ5Fgy9KlAziB6KHdOs08Lm/D46MvC1ZxPdyzwlp8StXDdCZ5aVhW1WTrrOLHcXn6kTNpKKY96SbsVlQfWXSHT8QO8aTcaP+qm4vV7ei3V0QR8aog99sQSeP8g8uS2mYTdvnblQKVNS43Vmga1vbq7bnqZpqaNDRQZlTkyKLi4rV5ZjbmZRNF0qMbe3130vyMuUP7kSMXWVL0GQ4tE3g6b2W9Y3XMyfX43SGRgNbuROVYwfmK9iv8OLm1EN/OwJguilBH/RiLk1FLK8uGcmMR345a1YSxu62wiTe6AV4UrJbtYJS31eRyTHh4h8OnDI9Rh5tmz0fJOI0YOUk9JG5fxtzgvHanYeDZyJGHl5M9bcXDHyyxoEQUoBhxjQ6+eGSMylddqJJAA0izyXPLsSZmkjGfsttjNIaeG24sCeNVGvIrNsnGy8W4hhibLnV6Ky0nNrN7XvMwZXXEM4wHmhjpgX2ce2xWZnKswspA5utq61HYigkOcycc8T015l5ubk2TuZoTGDlIEyrqYWF5F74d/4xLicPJlyKSdK9RQxUS7GVOzRqILlnxiivTpaoWL5Kzvp2FdXIqNaeoouYXca3OW0XKYc/offKzWnq3lbD/gM14RCyghV3tmQMnLnYkp8ZFZGqlzOKBQaTxto1WB2BTSlwGhKIWdIMVUcijEqLeSvmKaxr3ZJil2hjdLUqs6SUjPK0trMypZ2q2HZ+B07giDlg6qk1RiWTQAAACpJREFUScQIImIqK84GgogYlA2CcAZlgyCcQdkgCGdQNgjCGZQNgnDm/wAAAP//+3YdOwAAAAZJREFUAwCIUtlcw5Bs7QAAAABJRU5ErkJggg==",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langgraph.graph import START,END,StateGraph\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.prebuilt import tools_condition\n",
        "\n",
        "### Define a new graph\n",
        "\n",
        "workflow=StateGraph(AgentState)\n",
        "\n",
        "## Define the nodes we will cycle between\n",
        "\n",
        "workflow.add_node(\"agent\",agent) ## Agent\n",
        "retrive=ToolNode([retriever_tool,retriever_tool_langchain])\n",
        "workflow.add_node(\"retrieve\",retrive) ## Retrival\n",
        "workflow.add_node(\"rewrite\",rewrite) ## Re-Writing the question\n",
        "workflow.add_node(\n",
        "    \"generate\",generate\n",
        ") ## Generating  a response after we know the docuemnts are relavant\n",
        "\n",
        "## Call agent node to decide retrieve or not\n",
        "\n",
        "workflow.add_edge(START,\"agent\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tools_condition,\n",
        "    {\n",
        "        ## Translate the condtion output to node in our graph\n",
        "        \"tools\":\"retrieve\",\n",
        "        END:END,\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"retrieve\",\n",
        "    ## Assess agent decision\n",
        "    grade_documents,\n",
        ")\n",
        "workflow.add_edge(\"generate\",END)\n",
        "workflow.add_edge(\"rewrite\",\"agent\")\n",
        "\n",
        "graph=workflow.compile()\n",
        "\n",
        "from IPython.display import Image,display\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "7aa28d0d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--CALL AGENT ---\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='what is vector datbase and how to retrive?', additional_kwargs={}, response_metadata={}, id='2336cbd4-db07-435f-91e3-49835a17f4c0'),\n",
              "  AIMessage(content='**What is a Vector Database?**\\n\\nA vector database is a type of database designed to store and manage high-dimensional vector data, such as embeddings, dense vectors, or other types of numerical representations. These databases are optimized for similarity search, nearest neighbor search, and other operations that involve calculating distances or similarities between vectors.\\n\\nVector databases are commonly used in various applications, including:\\n\\n1. **Natural Language Processing (NLP)**: storing word embeddings, sentence embeddings, or document embeddings.\\n2. **Computer Vision**: storing image embeddings, feature vectors, or other visual representations.\\n3. **Recommendation Systems**: storing user embeddings, item embeddings, or other types of collaborative filtering data.\\n4. **Machine Learning**: storing model weights, activations, or other types of numerical data.\\n\\n**Key Characteristics of Vector Databases**\\n\\n1. **High-dimensional data**: Vector databases can handle high-dimensional data, often with thousands or millions of dimensions.\\n2. **Similarity search**: Vector databases are optimized for similarity search, which involves finding the most similar vectors to a given query vector.\\n3. **Nearest neighbor search**: Vector databases can efficiently find the nearest neighbors to a given query vector.\\n4. **Approximate search**: Vector databases often use approximate search algorithms to balance accuracy and performance.\\n\\n**Popular Vector Database Options**\\n\\n1. **Faiss** (Facebook AI Similarity Search): an open-source library for similarity search and clustering.\\n2. **Annoy** (Approximate Nearest Neighbors Oh Yeah!): a C++ library with Python bindings for efficient nearest neighbor search.\\n3. **Hnswlib** (Hierarchical Navigable Small World library): a C++ library with Python bindings for efficient nearest neighbor search.\\n4. **Milvus**: an open-source vector database designed for large-scale similarity search and AI applications.\\n5. **Pinecone**: a managed vector database service for building scalable and accurate AI models.\\n\\n**How to Retrieve Data from a Vector Database**\\n\\nThe retrieval process typically involves the following steps:\\n\\n1. **Indexing**: create an index of the vector data, which can be done using various indexing algorithms, such as hierarchical clustering or quantization.\\n2. **Querying**: provide a query vector and specify the search parameters, such as the number of nearest neighbors to retrieve.\\n3. **Search**: the vector database performs a similarity search or nearest neighbor search to find the most relevant vectors.\\n4. **Ranking**: the retrieved vectors are ranked according to their similarity or distance to the query vector.\\n\\n**Example Code (using Faiss)**\\n```python\\nimport numpy as np\\nimport faiss\\n\\n# Create a sample dataset\\nvectors = np.random.rand(100, 128).astype(\\'float32\\')\\n\\n# Create a Faiss index\\nindex = faiss.IndexFlatL2(128)  # L2 distance metric\\n\\n# Add vectors to the index\\nindex.add(vectors)\\n\\n# Define a query vector\\nquery_vector = np.random.rand(1, 128).astype(\\'float32\\')\\n\\n# Search for the 5 nearest neighbors\\nD, I = index.search(query_vector, 5)\\n\\n# Print the results\\nprint(\"Distances:\", D)\\nprint(\"Indices:\", I)\\n```\\nIn this example, we create a Faiss index, add some random vectors, and then search for the 5 nearest neighbors to a query vector. The `search` method returns the distances and indices of the nearest neighbors.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 702, 'prompt_tokens': 46, 'total_tokens': 748, 'completion_time': 1.627123778, 'prompt_time': 0.002245588, 'queue_time': 0.056358842, 'total_time': 1.6293693660000002}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_155ab82e98', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c7b5094f-949e-43bc-a548-0453d14563e1-0', usage_metadata={'input_tokens': 46, 'output_tokens': 702, 'total_tokens': 748})]}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph.invoke({\"messages\":\"what is vector datbase and how to retrive?\"})"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch-cuda12_8",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
